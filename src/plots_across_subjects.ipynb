{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d5f265e-407a-40bd-92fb-a652091fd7ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK  0\n",
      "PID of this process = 1113654\n",
      "device: cuda\n",
      "Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "\n",
    "from transformers import CLIPModel, AutoTokenizer, AutoProcessor\n",
    "import pandas as pd\n",
    "\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder\n",
    "from models import GNet8_Encoder, Clipper\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import utils\n",
    "\n",
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\") # ['no', 'fp8', 'fp16', 'bf16']\n",
    "\n",
    "print(\"PID of this process =\",os.getpid())\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0 or not distributed: num_devices = 1\n",
    "num_workers = num_devices\n",
    "print(accelerator.state)\n",
    "\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size)\n",
    "print = accelerator.print # only print if local_rank=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ffb659a-8154-4536-ab27-2d976da1bf4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: pretrained_subj01_40sess_hypatia_vd2\n",
      "--model_name=pretrained_subj01_40sess_hypatia_vd2 --data_path=/weka/proj-medarc/shared/mindeyev2_dataset --cache_dir=/weka/proj-medarc/shared/cache --all_recons_path=evals/pretrained_subj01_40sess_hypatia_vd2/pretrained_subj01_40sess_hypatia_vd2_all_recons_imagery.pt --mode imagery                         --imagery_data_path=/weka/proj-medarc/shared/umn-imagery --criteria=all\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name = \"pretrained_subj01_40sess_hypatia_vd2\"\n",
    "    # model_name = \"pretest_pretrained_subj01_40sess_hypatia_pg_sessions40\"\n",
    "    mode = \"imagery\"\n",
    "    # all_recons_path = f\"evals/{model_name}/{model_name}_all_enhancedrecons_{mode}.pt\"\n",
    "    all_recons_path = f\"evals/{model_name}/{model_name}_all_recons_{mode}.pt\"\n",
    "\n",
    "    cache_dir = \"/weka/proj-medarc/shared/cache\"\n",
    "    data_path = \"/weka/proj-medarc/shared/mindeyev2_dataset\"\n",
    "    criteria = \"all\"\n",
    "    # criteria = \"SSIM\"\n",
    "    print(\"model_name:\", model_name)\n",
    "\n",
    "    jupyter_args = f\"--model_name={model_name} --data_path={data_path} --cache_dir={cache_dir} --all_recons_path={all_recons_path} --mode {mode} \\\n",
    "                         --criteria={criteria}\"\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb8120cd-f226-4e2c-a6c5-3cd8ef6e9bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--all_recons_path\", type=str,\n",
    "    help=\"Path to where all_recons.pt is stored\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where misc. files downloaded from huggingface are stored. Defaults to current src directory.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subjs\",type=str, default='1,2,5,7',\n",
    "    help=\"Evaluate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mode\",type=str,default=\"vision\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--criteria\",type=str, default=\"all\",\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "\n",
    "criteria = criteria.replace(\"*\", \" \")\n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d66b33-b327-4895-a861-ecc6ccc51296",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loading tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "997f9672-b74d-4dcf-b4d7-a593fdce9cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if mode == \"synthetic\":\n",
    "    all_images = torch.zeros((284, 3, 714, 1360))\n",
    "    all_images[:220] = torch.load(f\"{data_path}/nsddata_stimuli/stimuli/nsdsynthetic/nsd_synthetic_stim_part1.pt\")\n",
    "    #The last 64 stimuli are slightly different for each subject, so we load these separately for each subject\n",
    "    all_images[220:] = torch.load(f\"{data_path}/nsddata_stimuli/stimuli/nsdsynthetic/nsd_synthetic_stim_part2_sub{subj}.pt\")\n",
    "else:\n",
    "    all_images = torch.load(f\"{data_path}/nsddata_stimuli/stimuli/imagery_stimuli_18.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a4f6f99-b1be-4924-b4d4-9785680219a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subjects: [1, 2, 5, 7]\n",
      "ms_model_names: ['pretrained_subj01_40sess_hypatia_vd2', 'pretrained_subj02_40sess_hypatia_vd2', 'pretrained_subj05_40sess_hypatia_vd2', 'pretrained_subj07_40sess_hypatia_vd2']\n",
      "ms_all_recons_paths: ['evals/pretrained_subj01_40sess_hypatia_vd2/pretrained_subj01_40sess_hypatia_vd2_all_recons_imagery.pt', 'evals/pretrained_subj02_40sess_hypatia_vd2/pretrained_subj02_40sess_hypatia_vd2_all_recons_imagery.pt', 'evals/pretrained_subj05_40sess_hypatia_vd2/pretrained_subj05_40sess_hypatia_vd2_all_recons_imagery.pt', 'evals/pretrained_subj07_40sess_hypatia_vd2/pretrained_subj07_40sess_hypatia_vd2_all_recons_imagery.pt']\n"
     ]
    }
   ],
   "source": [
    "subjects = [int(s) for s in subjs.split(\",\")]\n",
    "print(\"subjects:\", subjects)\n",
    "ms_model_names = []\n",
    "ms_all_recons_paths = []\n",
    "for subj in subjects:\n",
    "    m_name = model_name.replace(\"subj01\", f\"subj{subj:02d}\")\n",
    "    ms_model_names.append(m_name)\n",
    "    ms_all_recons_paths.append(all_recons_path.replace(\"subj01\", f\"subj{subj:02d}\"))    \n",
    "print(\"ms_model_names:\", ms_model_names)\n",
    "print(\"ms_all_recons_paths:\", ms_all_recons_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30fc0d00-3851-450e-8c3c-3d5005fe075a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded evals/pretrained_subj01_40sess_hypatia_vd2/pretrained_subj01_40sess_hypatia_vd2_all_recons_imagery.pt with shape torch.Size([18, 10, 3, 512, 512])\n",
      "Loaded tables/pretrained_subj01_40sess_hypatia_vd2_all_recons_imagery.csv\n",
      "Loaded evals/pretrained_subj02_40sess_hypatia_vd2/pretrained_subj02_40sess_hypatia_vd2_all_recons_imagery.pt with shape torch.Size([18, 10, 3, 512, 512])\n",
      "Loaded tables/pretrained_subj02_40sess_hypatia_vd2_all_recons_imagery.csv\n",
      "Loaded evals/pretrained_subj05_40sess_hypatia_vd2/pretrained_subj05_40sess_hypatia_vd2_all_recons_imagery.pt with shape torch.Size([18, 10, 3, 512, 512])\n",
      "Loaded tables/pretrained_subj05_40sess_hypatia_vd2_all_recons_imagery.csv\n",
      "Loaded evals/pretrained_subj07_40sess_hypatia_vd2/pretrained_subj07_40sess_hypatia_vd2_all_recons_imagery.pt with shape torch.Size([18, 10, 3, 512, 512])\n",
      "Loaded tables/pretrained_subj07_40sess_hypatia_vd2_all_recons_imagery.csv\n"
     ]
    }
   ],
   "source": [
    "ms_all_metrics = []\n",
    "ms_all_recons = []\n",
    "\n",
    "for m_name, m_all_recons_path, subj in zip(ms_model_names, ms_all_recons_paths, subjects):\n",
    "    all_recons = torch.load(m_all_recons_path)\n",
    "    ms_all_recons.append(all_recons.reshape((all_recons.shape[0], 10, 3, 512, 512)))\n",
    "    print(f\"Loaded {m_all_recons_path} with shape {all_recons.shape}\")\n",
    "    \n",
    "    all_metric_table = pd.read_csv(f\"tables/{m_name}_all_recons_{mode}.csv\", sep=\"\\t\")\n",
    "\n",
    "    # add a column with all values the subject \n",
    "    all_metric_table[\"Subject\"] = [subj]*len(all_metric_table)\n",
    "\n",
    "    ms_all_metrics.append(all_metric_table)\n",
    "    print(f\"Loaded tables/{m_name}_all_recons_{mode}.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af4f2c51-8a1d-4605-9ac4-d3185b77e863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-ckadirt/mindeye/lib/python3.11/site-packages/sklearn/utils/_array_api.py:472: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/admin/home-ckadirt/mindeye/lib/python3.11/site-packages/sklearn/utils/_array_api.py:489: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n"
     ]
    }
   ],
   "source": [
    "# concatename all the metrics vertically\n",
    "all_metrics = pd.concat(ms_all_metrics)\n",
    "# print(all_metrics)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "columns_to_normalize = [\n",
    "    'PixCorr', 'SSIM', 'AlexNet(2)', 'AlexNet(5)', 'InceptionV3', 'CLIP', \n",
    "    'EffNet-B', 'SwAV', 'FwdRetrieval', 'BwdRetrieval', 'Brain Corr. nsd_general',\n",
    "    'Brain Corr. V1', 'Brain Corr. V2', 'Brain Corr. V3', 'Brain Corr. V4', \n",
    "    'Brain Corr. higher_vis'\n",
    "]\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply normalization only to the columns that need it\n",
    "all_metrics[columns_to_normalize] = scaler.fit_transform(all_metrics[columns_to_normalize])\n",
    "# print(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "782f6153-2ba6-43dd-b5ca-861da795bf74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index_sample</th>\n",
       "      <th>PixCorr</th>\n",
       "      <th>SSIM</th>\n",
       "      <th>AlexNet(2)</th>\n",
       "      <th>AlexNet(5)</th>\n",
       "      <th>InceptionV3</th>\n",
       "      <th>CLIP</th>\n",
       "      <th>EffNet-B</th>\n",
       "      <th>SwAV</th>\n",
       "      <th>FwdRetrieval</th>\n",
       "      <th>BwdRetrieval</th>\n",
       "      <th>Brain Corr. nsd_general</th>\n",
       "      <th>Brain Corr. V1</th>\n",
       "      <th>Brain Corr. V2</th>\n",
       "      <th>Brain Corr. V3</th>\n",
       "      <th>Brain Corr. V4</th>\n",
       "      <th>Brain Corr. higher_vis</th>\n",
       "      <th>index_image</th>\n",
       "      <th>Subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.428476</td>\n",
       "      <td>0.454252</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.860549</td>\n",
       "      <td>0.510936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.631969</td>\n",
       "      <td>0.606122</td>\n",
       "      <td>0.616936</td>\n",
       "      <td>0.802548</td>\n",
       "      <td>0.594443</td>\n",
       "      <td>0.512541</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.525483</td>\n",
       "      <td>0.508542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.802844</td>\n",
       "      <td>0.379130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.241625</td>\n",
       "      <td>0.221836</td>\n",
       "      <td>0.191426</td>\n",
       "      <td>0.403708</td>\n",
       "      <td>0.585154</td>\n",
       "      <td>0.166032</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0.442006</td>\n",
       "      <td>0.399694</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.762465</td>\n",
       "      <td>0.561037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.547885</td>\n",
       "      <td>0.520267</td>\n",
       "      <td>0.580332</td>\n",
       "      <td>0.708245</td>\n",
       "      <td>0.376977</td>\n",
       "      <td>0.439004</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>0.452865</td>\n",
       "      <td>0.435139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.671867</td>\n",
       "      <td>0.388977</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.578202</td>\n",
       "      <td>0.577559</td>\n",
       "      <td>0.652961</td>\n",
       "      <td>0.854588</td>\n",
       "      <td>0.452949</td>\n",
       "      <td>0.435165</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>0.484365</td>\n",
       "      <td>0.569934</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.794539</td>\n",
       "      <td>0.385904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.364344</td>\n",
       "      <td>0.426512</td>\n",
       "      <td>0.405006</td>\n",
       "      <td>0.418732</td>\n",
       "      <td>0.442123</td>\n",
       "      <td>0.271767</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>0.593301</td>\n",
       "      <td>0.570448</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.697656</td>\n",
       "      <td>0.440843</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.359457</td>\n",
       "      <td>0.349316</td>\n",
       "      <td>0.311249</td>\n",
       "      <td>0.382540</td>\n",
       "      <td>0.435627</td>\n",
       "      <td>0.300006</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>6</td>\n",
       "      <td>0.534202</td>\n",
       "      <td>0.608789</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.861229</td>\n",
       "      <td>0.348104</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.288485</td>\n",
       "      <td>0.289117</td>\n",
       "      <td>0.295454</td>\n",
       "      <td>0.506409</td>\n",
       "      <td>0.610947</td>\n",
       "      <td>0.193320</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>84</td>\n",
       "      <td>7</td>\n",
       "      <td>0.491064</td>\n",
       "      <td>0.498523</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.746052</td>\n",
       "      <td>0.448572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.332578</td>\n",
       "      <td>0.412759</td>\n",
       "      <td>0.380771</td>\n",
       "      <td>0.653806</td>\n",
       "      <td>0.537002</td>\n",
       "      <td>0.168778</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>8</td>\n",
       "      <td>0.449444</td>\n",
       "      <td>0.412133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.720851</td>\n",
       "      <td>0.507653</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.484133</td>\n",
       "      <td>0.534624</td>\n",
       "      <td>0.561522</td>\n",
       "      <td>0.652858</td>\n",
       "      <td>0.419985</td>\n",
       "      <td>0.356660</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>108</td>\n",
       "      <td>9</td>\n",
       "      <td>0.471510</td>\n",
       "      <td>0.567247</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.727296</td>\n",
       "      <td>0.398313</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.554814</td>\n",
       "      <td>0.542982</td>\n",
       "      <td>0.540724</td>\n",
       "      <td>0.750104</td>\n",
       "      <td>0.531016</td>\n",
       "      <td>0.456753</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.479995</td>\n",
       "      <td>0.957144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.776994</td>\n",
       "      <td>0.434594</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.649730</td>\n",
       "      <td>0.730928</td>\n",
       "      <td>0.724617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959141</td>\n",
       "      <td>0.415595</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.293429</td>\n",
       "      <td>0.892156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.741060</td>\n",
       "      <td>0.412425</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.488315</td>\n",
       "      <td>0.620004</td>\n",
       "      <td>0.554490</td>\n",
       "      <td>0.352853</td>\n",
       "      <td>0.415734</td>\n",
       "      <td>0.411986</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0.430367</td>\n",
       "      <td>0.859597</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.805773</td>\n",
       "      <td>0.529091</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.441132</td>\n",
       "      <td>0.617203</td>\n",
       "      <td>0.573711</td>\n",
       "      <td>0.573815</td>\n",
       "      <td>0.412650</td>\n",
       "      <td>0.385006</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>0.197053</td>\n",
       "      <td>0.787558</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.676773</td>\n",
       "      <td>0.463414</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.404094</td>\n",
       "      <td>0.461414</td>\n",
       "      <td>0.434718</td>\n",
       "      <td>0.194460</td>\n",
       "      <td>0.286671</td>\n",
       "      <td>0.358558</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>0.544734</td>\n",
       "      <td>0.887186</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.609405</td>\n",
       "      <td>0.473814</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.429129</td>\n",
       "      <td>0.526101</td>\n",
       "      <td>0.428436</td>\n",
       "      <td>0.229805</td>\n",
       "      <td>0.182081</td>\n",
       "      <td>0.445943</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>0.316961</td>\n",
       "      <td>0.902526</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.653447</td>\n",
       "      <td>0.435786</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.580213</td>\n",
       "      <td>0.651690</td>\n",
       "      <td>0.733225</td>\n",
       "      <td>0.856819</td>\n",
       "      <td>0.839929</td>\n",
       "      <td>0.356646</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>6</td>\n",
       "      <td>0.361791</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.812938</td>\n",
       "      <td>0.378855</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.534148</td>\n",
       "      <td>0.743798</td>\n",
       "      <td>0.667748</td>\n",
       "      <td>0.630602</td>\n",
       "      <td>0.536061</td>\n",
       "      <td>0.412606</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>84</td>\n",
       "      <td>7</td>\n",
       "      <td>0.393029</td>\n",
       "      <td>0.910335</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.764888</td>\n",
       "      <td>0.552477</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.565563</td>\n",
       "      <td>0.675953</td>\n",
       "      <td>0.666696</td>\n",
       "      <td>0.699250</td>\n",
       "      <td>0.620250</td>\n",
       "      <td>0.392420</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>8</td>\n",
       "      <td>0.353422</td>\n",
       "      <td>0.854716</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.567032</td>\n",
       "      <td>0.546000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.433280</td>\n",
       "      <td>0.563256</td>\n",
       "      <td>0.483490</td>\n",
       "      <td>0.278522</td>\n",
       "      <td>0.223730</td>\n",
       "      <td>0.408402</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>108</td>\n",
       "      <td>9</td>\n",
       "      <td>0.342738</td>\n",
       "      <td>0.916167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.692008</td>\n",
       "      <td>0.360359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.425030</td>\n",
       "      <td>0.667074</td>\n",
       "      <td>0.528837</td>\n",
       "      <td>0.167286</td>\n",
       "      <td>0.473754</td>\n",
       "      <td>0.358531</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.510669</td>\n",
       "      <td>0.517808</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.789623</td>\n",
       "      <td>0.505423</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.317141</td>\n",
       "      <td>0.700161</td>\n",
       "      <td>0.334395</td>\n",
       "      <td>0.228459</td>\n",
       "      <td>0.417188</td>\n",
       "      <td>0.258442</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.511159</td>\n",
       "      <td>0.724382</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.768939</td>\n",
       "      <td>0.613199</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.283858</td>\n",
       "      <td>0.648414</td>\n",
       "      <td>0.288589</td>\n",
       "      <td>0.149698</td>\n",
       "      <td>0.574502</td>\n",
       "      <td>0.217534</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0.470444</td>\n",
       "      <td>0.682397</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.732533</td>\n",
       "      <td>0.445555</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.266493</td>\n",
       "      <td>0.630788</td>\n",
       "      <td>0.234942</td>\n",
       "      <td>0.104186</td>\n",
       "      <td>0.514492</td>\n",
       "      <td>0.228411</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>0.536325</td>\n",
       "      <td>0.616909</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.700711</td>\n",
       "      <td>0.470748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.355807</td>\n",
       "      <td>0.617853</td>\n",
       "      <td>0.525556</td>\n",
       "      <td>0.426564</td>\n",
       "      <td>0.411865</td>\n",
       "      <td>0.247564</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>0.467017</td>\n",
       "      <td>0.604603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.726556</td>\n",
       "      <td>0.474535</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.375574</td>\n",
       "      <td>0.762152</td>\n",
       "      <td>0.380308</td>\n",
       "      <td>0.248773</td>\n",
       "      <td>0.476316</td>\n",
       "      <td>0.311654</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>0.589001</td>\n",
       "      <td>0.722286</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.811210</td>\n",
       "      <td>0.420137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.328612</td>\n",
       "      <td>0.766741</td>\n",
       "      <td>0.520321</td>\n",
       "      <td>0.365507</td>\n",
       "      <td>0.582871</td>\n",
       "      <td>0.162648</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>6</td>\n",
       "      <td>0.562536</td>\n",
       "      <td>0.582326</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.751507</td>\n",
       "      <td>0.645213</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.307457</td>\n",
       "      <td>0.448846</td>\n",
       "      <td>0.165064</td>\n",
       "      <td>0.231526</td>\n",
       "      <td>0.642680</td>\n",
       "      <td>0.290287</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>84</td>\n",
       "      <td>7</td>\n",
       "      <td>0.522362</td>\n",
       "      <td>0.583918</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.727489</td>\n",
       "      <td>0.403368</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.388821</td>\n",
       "      <td>0.680924</td>\n",
       "      <td>0.344031</td>\n",
       "      <td>0.272688</td>\n",
       "      <td>0.458766</td>\n",
       "      <td>0.345575</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>8</td>\n",
       "      <td>0.547640</td>\n",
       "      <td>0.736817</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.752867</td>\n",
       "      <td>0.614509</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.265688</td>\n",
       "      <td>0.676764</td>\n",
       "      <td>0.225163</td>\n",
       "      <td>0.100837</td>\n",
       "      <td>0.511731</td>\n",
       "      <td>0.222573</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>108</td>\n",
       "      <td>9</td>\n",
       "      <td>0.508331</td>\n",
       "      <td>0.531011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.798431</td>\n",
       "      <td>0.482575</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.318600</td>\n",
       "      <td>0.631118</td>\n",
       "      <td>0.345784</td>\n",
       "      <td>0.269387</td>\n",
       "      <td>0.476667</td>\n",
       "      <td>0.250740</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524669</td>\n",
       "      <td>0.373138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.722701</td>\n",
       "      <td>0.586300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.680294</td>\n",
       "      <td>0.853075</td>\n",
       "      <td>0.721440</td>\n",
       "      <td>0.615712</td>\n",
       "      <td>0.624787</td>\n",
       "      <td>0.521965</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.573676</td>\n",
       "      <td>0.531727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.834519</td>\n",
       "      <td>0.307265</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.667911</td>\n",
       "      <td>0.907371</td>\n",
       "      <td>0.718103</td>\n",
       "      <td>0.691989</td>\n",
       "      <td>0.763859</td>\n",
       "      <td>0.484868</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0.452926</td>\n",
       "      <td>0.404038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.810183</td>\n",
       "      <td>0.420728</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.677775</td>\n",
       "      <td>0.852088</td>\n",
       "      <td>0.710696</td>\n",
       "      <td>0.662993</td>\n",
       "      <td>0.766126</td>\n",
       "      <td>0.510262</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>0.496355</td>\n",
       "      <td>0.488281</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.853294</td>\n",
       "      <td>0.441428</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.653442</td>\n",
       "      <td>0.812192</td>\n",
       "      <td>0.663972</td>\n",
       "      <td>0.636694</td>\n",
       "      <td>0.654487</td>\n",
       "      <td>0.501626</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>0.406838</td>\n",
       "      <td>0.515297</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.883431</td>\n",
       "      <td>0.315814</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.601710</td>\n",
       "      <td>0.876802</td>\n",
       "      <td>0.688569</td>\n",
       "      <td>0.679723</td>\n",
       "      <td>0.756139</td>\n",
       "      <td>0.423239</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>0.429984</td>\n",
       "      <td>0.514692</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.859536</td>\n",
       "      <td>0.293999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.693302</td>\n",
       "      <td>0.815387</td>\n",
       "      <td>0.694117</td>\n",
       "      <td>0.728595</td>\n",
       "      <td>0.820128</td>\n",
       "      <td>0.536124</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>6</td>\n",
       "      <td>0.545270</td>\n",
       "      <td>0.432817</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.783791</td>\n",
       "      <td>0.441780</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.668956</td>\n",
       "      <td>0.838943</td>\n",
       "      <td>0.726821</td>\n",
       "      <td>0.718380</td>\n",
       "      <td>0.765077</td>\n",
       "      <td>0.493371</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>84</td>\n",
       "      <td>7</td>\n",
       "      <td>0.488207</td>\n",
       "      <td>0.427815</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.867923</td>\n",
       "      <td>0.592904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.684354</td>\n",
       "      <td>0.771420</td>\n",
       "      <td>0.688072</td>\n",
       "      <td>0.635884</td>\n",
       "      <td>0.637335</td>\n",
       "      <td>0.546043</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>8</td>\n",
       "      <td>0.487903</td>\n",
       "      <td>0.356635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.895588</td>\n",
       "      <td>0.542539</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.689244</td>\n",
       "      <td>0.873824</td>\n",
       "      <td>0.735720</td>\n",
       "      <td>0.678479</td>\n",
       "      <td>0.777453</td>\n",
       "      <td>0.526400</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>108</td>\n",
       "      <td>9</td>\n",
       "      <td>0.469784</td>\n",
       "      <td>0.423088</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.889094</td>\n",
       "      <td>0.373111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.626255</td>\n",
       "      <td>0.716555</td>\n",
       "      <td>0.595656</td>\n",
       "      <td>0.589723</td>\n",
       "      <td>0.746575</td>\n",
       "      <td>0.488465</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  index_sample   PixCorr      SSIM  AlexNet(2)  AlexNet(5)  \\\n",
       "0             0             0  0.428476  0.454252         NaN         NaN   \n",
       "12           12             1  0.525483  0.508542         NaN         NaN   \n",
       "24           24             2  0.442006  0.399694         NaN         NaN   \n",
       "36           36             3  0.452865  0.435139         NaN         NaN   \n",
       "48           48             4  0.484365  0.569934         NaN         NaN   \n",
       "60           60             5  0.593301  0.570448         NaN         NaN   \n",
       "72           72             6  0.534202  0.608789         NaN         NaN   \n",
       "84           84             7  0.491064  0.498523         NaN         NaN   \n",
       "96           96             8  0.449444  0.412133         NaN         NaN   \n",
       "108         108             9  0.471510  0.567247         NaN         NaN   \n",
       "0             0             0  0.479995  0.957144         NaN         NaN   \n",
       "12           12             1  0.293429  0.892156         NaN         NaN   \n",
       "24           24             2  0.430367  0.859597         NaN         NaN   \n",
       "36           36             3  0.197053  0.787558         NaN         NaN   \n",
       "48           48             4  0.544734  0.887186         NaN         NaN   \n",
       "60           60             5  0.316961  0.902526         NaN         NaN   \n",
       "72           72             6  0.361791  1.000000         NaN         NaN   \n",
       "84           84             7  0.393029  0.910335         NaN         NaN   \n",
       "96           96             8  0.353422  0.854716         NaN         NaN   \n",
       "108         108             9  0.342738  0.916167         NaN         NaN   \n",
       "0             0             0  0.510669  0.517808         NaN         NaN   \n",
       "12           12             1  0.511159  0.724382         NaN         NaN   \n",
       "24           24             2  0.470444  0.682397         NaN         NaN   \n",
       "36           36             3  0.536325  0.616909         NaN         NaN   \n",
       "48           48             4  0.467017  0.604603         NaN         NaN   \n",
       "60           60             5  0.589001  0.722286         NaN         NaN   \n",
       "72           72             6  0.562536  0.582326         NaN         NaN   \n",
       "84           84             7  0.522362  0.583918         NaN         NaN   \n",
       "96           96             8  0.547640  0.736817         NaN         NaN   \n",
       "108         108             9  0.508331  0.531011         NaN         NaN   \n",
       "0             0             0  0.524669  0.373138         NaN         NaN   \n",
       "12           12             1  0.573676  0.531727         NaN         NaN   \n",
       "24           24             2  0.452926  0.404038         NaN         NaN   \n",
       "36           36             3  0.496355  0.488281         NaN         NaN   \n",
       "48           48             4  0.406838  0.515297         NaN         NaN   \n",
       "60           60             5  0.429984  0.514692         NaN         NaN   \n",
       "72           72             6  0.545270  0.432817         NaN         NaN   \n",
       "84           84             7  0.488207  0.427815         NaN         NaN   \n",
       "96           96             8  0.487903  0.356635         NaN         NaN   \n",
       "108         108             9  0.469784  0.423088         NaN         NaN   \n",
       "\n",
       "     InceptionV3  CLIP  EffNet-B      SwAV  FwdRetrieval  BwdRetrieval  \\\n",
       "0            NaN   NaN  0.860549  0.510936           NaN           NaN   \n",
       "12           NaN   NaN  0.802844  0.379130           NaN           NaN   \n",
       "24           NaN   NaN  0.762465  0.561037           NaN           NaN   \n",
       "36           NaN   NaN  0.671867  0.388977           NaN           NaN   \n",
       "48           NaN   NaN  0.794539  0.385904           NaN           NaN   \n",
       "60           NaN   NaN  0.697656  0.440843           NaN           NaN   \n",
       "72           NaN   NaN  0.861229  0.348104           NaN           NaN   \n",
       "84           NaN   NaN  0.746052  0.448572           NaN           NaN   \n",
       "96           NaN   NaN  0.720851  0.507653           NaN           NaN   \n",
       "108          NaN   NaN  0.727296  0.398313           NaN           NaN   \n",
       "0            NaN   NaN  0.776994  0.434594           NaN           NaN   \n",
       "12           NaN   NaN  0.741060  0.412425           NaN           NaN   \n",
       "24           NaN   NaN  0.805773  0.529091           NaN           NaN   \n",
       "36           NaN   NaN  0.676773  0.463414           NaN           NaN   \n",
       "48           NaN   NaN  0.609405  0.473814           NaN           NaN   \n",
       "60           NaN   NaN  0.653447  0.435786           NaN           NaN   \n",
       "72           NaN   NaN  0.812938  0.378855           NaN           NaN   \n",
       "84           NaN   NaN  0.764888  0.552477           NaN           NaN   \n",
       "96           NaN   NaN  0.567032  0.546000           NaN           NaN   \n",
       "108          NaN   NaN  0.692008  0.360359           NaN           NaN   \n",
       "0            NaN   NaN  0.789623  0.505423           NaN           NaN   \n",
       "12           NaN   NaN  0.768939  0.613199           NaN           NaN   \n",
       "24           NaN   NaN  0.732533  0.445555           NaN           NaN   \n",
       "36           NaN   NaN  0.700711  0.470748           NaN           NaN   \n",
       "48           NaN   NaN  0.726556  0.474535           NaN           NaN   \n",
       "60           NaN   NaN  0.811210  0.420137           NaN           NaN   \n",
       "72           NaN   NaN  0.751507  0.645213           NaN           NaN   \n",
       "84           NaN   NaN  0.727489  0.403368           NaN           NaN   \n",
       "96           NaN   NaN  0.752867  0.614509           NaN           NaN   \n",
       "108          NaN   NaN  0.798431  0.482575           NaN           NaN   \n",
       "0            NaN   NaN  0.722701  0.586300           NaN           NaN   \n",
       "12           NaN   NaN  0.834519  0.307265           NaN           NaN   \n",
       "24           NaN   NaN  0.810183  0.420728           NaN           NaN   \n",
       "36           NaN   NaN  0.853294  0.441428           NaN           NaN   \n",
       "48           NaN   NaN  0.883431  0.315814           NaN           NaN   \n",
       "60           NaN   NaN  0.859536  0.293999           NaN           NaN   \n",
       "72           NaN   NaN  0.783791  0.441780           NaN           NaN   \n",
       "84           NaN   NaN  0.867923  0.592904           NaN           NaN   \n",
       "96           NaN   NaN  0.895588  0.542539           NaN           NaN   \n",
       "108          NaN   NaN  0.889094  0.373111           NaN           NaN   \n",
       "\n",
       "     Brain Corr. nsd_general  Brain Corr. V1  Brain Corr. V2  Brain Corr. V3  \\\n",
       "0                   0.631969        0.606122        0.616936        0.802548   \n",
       "12                  0.241625        0.221836        0.191426        0.403708   \n",
       "24                  0.547885        0.520267        0.580332        0.708245   \n",
       "36                  0.578202        0.577559        0.652961        0.854588   \n",
       "48                  0.364344        0.426512        0.405006        0.418732   \n",
       "60                  0.359457        0.349316        0.311249        0.382540   \n",
       "72                  0.288485        0.289117        0.295454        0.506409   \n",
       "84                  0.332578        0.412759        0.380771        0.653806   \n",
       "96                  0.484133        0.534624        0.561522        0.652858   \n",
       "108                 0.554814        0.542982        0.540724        0.750104   \n",
       "0                   0.649730        0.730928        0.724617        1.000000   \n",
       "12                  0.488315        0.620004        0.554490        0.352853   \n",
       "24                  0.441132        0.617203        0.573711        0.573815   \n",
       "36                  0.404094        0.461414        0.434718        0.194460   \n",
       "48                  0.429129        0.526101        0.428436        0.229805   \n",
       "60                  0.580213        0.651690        0.733225        0.856819   \n",
       "72                  0.534148        0.743798        0.667748        0.630602   \n",
       "84                  0.565563        0.675953        0.666696        0.699250   \n",
       "96                  0.433280        0.563256        0.483490        0.278522   \n",
       "108                 0.425030        0.667074        0.528837        0.167286   \n",
       "0                   0.317141        0.700161        0.334395        0.228459   \n",
       "12                  0.283858        0.648414        0.288589        0.149698   \n",
       "24                  0.266493        0.630788        0.234942        0.104186   \n",
       "36                  0.355807        0.617853        0.525556        0.426564   \n",
       "48                  0.375574        0.762152        0.380308        0.248773   \n",
       "60                  0.328612        0.766741        0.520321        0.365507   \n",
       "72                  0.307457        0.448846        0.165064        0.231526   \n",
       "84                  0.388821        0.680924        0.344031        0.272688   \n",
       "96                  0.265688        0.676764        0.225163        0.100837   \n",
       "108                 0.318600        0.631118        0.345784        0.269387   \n",
       "0                   0.680294        0.853075        0.721440        0.615712   \n",
       "12                  0.667911        0.907371        0.718103        0.691989   \n",
       "24                  0.677775        0.852088        0.710696        0.662993   \n",
       "36                  0.653442        0.812192        0.663972        0.636694   \n",
       "48                  0.601710        0.876802        0.688569        0.679723   \n",
       "60                  0.693302        0.815387        0.694117        0.728595   \n",
       "72                  0.668956        0.838943        0.726821        0.718380   \n",
       "84                  0.684354        0.771420        0.688072        0.635884   \n",
       "96                  0.689244        0.873824        0.735720        0.678479   \n",
       "108                 0.626255        0.716555        0.595656        0.589723   \n",
       "\n",
       "     Brain Corr. V4  Brain Corr. higher_vis  index_image  Subject  \n",
       "0          0.594443                0.512541            0        1  \n",
       "12         0.585154                0.166032            0        1  \n",
       "24         0.376977                0.439004            0        1  \n",
       "36         0.452949                0.435165            0        1  \n",
       "48         0.442123                0.271767            0        1  \n",
       "60         0.435627                0.300006            0        1  \n",
       "72         0.610947                0.193320            0        1  \n",
       "84         0.537002                0.168778            0        1  \n",
       "96         0.419985                0.356660            0        1  \n",
       "108        0.531016                0.456753            0        1  \n",
       "0          0.959141                0.415595            0        2  \n",
       "12         0.415734                0.411986            0        2  \n",
       "24         0.412650                0.385006            0        2  \n",
       "36         0.286671                0.358558            0        2  \n",
       "48         0.182081                0.445943            0        2  \n",
       "60         0.839929                0.356646            0        2  \n",
       "72         0.536061                0.412606            0        2  \n",
       "84         0.620250                0.392420            0        2  \n",
       "96         0.223730                0.408402            0        2  \n",
       "108        0.473754                0.358531            0        2  \n",
       "0          0.417188                0.258442            0        5  \n",
       "12         0.574502                0.217534            0        5  \n",
       "24         0.514492                0.228411            0        5  \n",
       "36         0.411865                0.247564            0        5  \n",
       "48         0.476316                0.311654            0        5  \n",
       "60         0.582871                0.162648            0        5  \n",
       "72         0.642680                0.290287            0        5  \n",
       "84         0.458766                0.345575            0        5  \n",
       "96         0.511731                0.222573            0        5  \n",
       "108        0.476667                0.250740            0        5  \n",
       "0          0.624787                0.521965            0        7  \n",
       "12         0.763859                0.484868            0        7  \n",
       "24         0.766126                0.510262            0        7  \n",
       "36         0.654487                0.501626            0        7  \n",
       "48         0.756139                0.423239            0        7  \n",
       "60         0.820128                0.536124            0        7  \n",
       "72         0.765077                0.493371            0        7  \n",
       "84         0.637335                0.546043            0        7  \n",
       "96         0.777453                0.526400            0        7  \n",
       "108        0.746575                0.488465            0        7  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metrics[all_metrics['index_image'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4ced68b-bf8a-4218-ad0f-f166e03c7f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_and_medium(df, criteria):\n",
    "    # Columns to ignore\n",
    "    ignore_columns = []\n",
    "    # Drop the columns to ignore from the DataFrame\n",
    "    df_filtered = df.drop(columns=ignore_columns, errors='ignore')\n",
    "    \n",
    "    if criteria == \"all\":\n",
    "        # Average all metrics except the ignored columns\n",
    "        all_metrics = [\n",
    "            'PixCorr', 'SSIM', 'AlexNet(2)', 'AlexNet(5)', 'InceptionV3', 'CLIP', \n",
    "            'EffNet-B', 'SwAV', 'FwdRetrieval', 'BwdRetrieval', \n",
    "            'Brain Corr. nsd_general',\n",
    "            'Brain Corr. V1', 'Brain Corr. V2', 'Brain Corr. V3', 'Brain Corr. V4', \n",
    "            'Brain Corr. higher_vis'\n",
    "        ]\n",
    "        scores = df_filtered[all_metrics].mean(axis=1)\n",
    "    elif isinstance(criteria, str):\n",
    "        # Handle the case where criteria is a single string\n",
    "        scores = df_filtered[criteria]\n",
    "    else:\n",
    "        # Handle the case where criteria is a list of columns\n",
    "        scores = df_filtered[criteria].mean(axis=1)\n",
    "    \n",
    "    # Get the index of the best score (highest)\n",
    "    best_index = scores.idxmax()\n",
    "    \n",
    "    # Get the index of the median score\n",
    "    median_index = scores.sort_values().index[len(scores) // 2]\n",
    "    \n",
    "    # Return the best and median rows\n",
    "    best_row = df.iloc[best_index]\n",
    "    median_row = df.iloc[median_index]\n",
    "    \n",
    "    return best_row, median_row\n",
    "\n",
    "# Example usage:\n",
    "# criteria = [\"AlexNet(2)\", \"SSIM\"]  # list of columns\n",
    "# criteria = \"AlexNet(2)\"  # single string\n",
    "# best_row, median_row = get_best_and_medium(df, criteria)\n",
    "\n",
    "# print(\"Best sample row:\\n\", best_row)\n",
    "# print(\"Median sample row:\\n\", median_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6eb11d9-04d0-450d-b592-fb4fe8182647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_image_metrics['Brain Corr. nsd_general']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bd842b8-3782-46cf-a57d-81e41fc2eb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_row_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35405ae1-58a1-44a3-b856-58361fc2405e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0 0.0\n",
      "7.0 8.0\n",
      "2.0 6.0\n",
      "2.0 8.0\n",
      "1.0 3.0\n",
      "7.0 3.0\n",
      "2.0 6.0\n",
      "2.0 8.0\n",
      "2.0 4.0\n",
      "2.0 8.0\n",
      "2.0 1.0\n",
      "1.0 4.0\n"
     ]
    }
   ],
   "source": [
    "selected_best_final_images = []\n",
    "selected_median_final_images = []\n",
    "for current_image_index, i in enumerate(all_images[:12]):\n",
    "    # select all the rows with the current image index\n",
    "    current_image_metrics = all_metrics[all_metrics[\"index_image\"] == current_image_index].reset_index()\n",
    "    # convert current_image_metrics to a dataframe\n",
    "    # current_image_metrics = pd.DataFrame(current_image_metrics)\n",
    "\n",
    "    selected_row_best, selected_row_median = get_best_and_medium(current_image_metrics, criteria)\n",
    "\n",
    "    # selected_row_best = current_image_metrics.loc[best_index]\n",
    "    # print(selected_row_best)\n",
    "    # selected_row_median = current_image_metrics.loc[median_index]\n",
    "\n",
    "    selected_subject_best = selected_row_best[\"Subject\"]\n",
    "    selected_subject_median = selected_row_median[\"Subject\"]\n",
    "\n",
    "    # this is in the second column of the table\n",
    "    selected_sample_best = selected_row_best[\"index_sample\"]\n",
    "    selected_sample_median = selected_row_median[\"index_sample\"]\n",
    "\n",
    "    selected_subject_best_index = subjects.index(selected_subject_best)\n",
    "    selected_subject_median_index = subjects.index(selected_subject_median)\n",
    "\n",
    "    # print(ms_all_recons[selected_subject_best_index].shape, selected_sample_best, current_image_index)\n",
    "    print(selected_subject_best, selected_sample_best)\n",
    "    selected_best_final_images.append(ms_all_recons[selected_subject_best_index][current_image_index][int(selected_sample_best)])\n",
    "    selected_median_final_images.append(ms_all_recons[selected_subject_median_index][current_image_index][int(selected_sample_median)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b085043-f4fc-4ce9-a54c-0d682bc6bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_image_metrics = all_metrics[all_metrics[\"index_image\"] == 0].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8572cc0b-77cd-4bb8-b532-31419712e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ms_all_recons[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45f4526d-fe42-4396-987f-d562573175c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_image_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36fbb7b8-3f8a-4446-9686-145dc8a5c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_recons_best = torch.stack(selected_best_final_images)\n",
    "all_recons_median = torch.stack(selected_median_final_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cefc34b-d14c-485d-b029-0a22c98d39bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 3, 425, 425])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff10f9c3-85a1-4cfa-87c0-6211619febeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "imsize = 150\n",
    "\n",
    "def resize_with_padding(image, target_size):\n",
    "    # Get the size difference between target size and current image size\n",
    "    current_size = image.shape[-2:]\n",
    "    padding = [max(0, (target_size[i] - current_size[i])) // 2 for i in range(2)]\n",
    "    \n",
    "    # Pad both sides equally (top/bottom and left/right)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(target_size),  # Resize to target size while maintaining aspect ratio\n",
    "        transforms.Pad((padding[1], padding[0], padding[1], padding[0]), fill=0)  # Pad to target size, avoiding negative padding\n",
    "    ])\n",
    "    return transform(image)\n",
    "\n",
    "def save_plot(all_images, all_recons, name):\n",
    "    # Resize images without cropping and maintain aspect ratio with padding\n",
    "    if all_images.shape[-1] != imsize:\n",
    "        all_images = resize_with_padding(all_images, (imsize, imsize)).float()\n",
    "    if all_recons.shape[-1] != imsize:\n",
    "        all_recons = resize_with_padding(all_recons, (imsize, imsize)).float()\n",
    "    \n",
    "    num_images = all_recons.shape[0]\n",
    "    num_rows = (2 * num_images + 11) // 12\n",
    "    \n",
    "    # Interleave tensors\n",
    "    merged = torch.stack([val for pair in zip(all_images, all_recons) for val in pair], dim=0)\n",
    "    \n",
    "    # Calculate grid size\n",
    "    grid = torch.zeros((num_rows * 12, 3, imsize, imsize))\n",
    "    \n",
    "    # Populate the grid\n",
    "    grid[:2*num_images] = merged\n",
    "    grid_images = [transforms.functional.to_pil_image(grid[i]) for i in range(num_rows * 12)]\n",
    "    \n",
    "    # Create the grid image\n",
    "    grid_image = Image.new('RGB', (imsize * 12, imsize * num_rows))  # 12 images wide\n",
    "    \n",
    "    # Paste images into the grid\n",
    "    for i, img in enumerate(grid_images):\n",
    "        grid_image.paste(img, (imsize * (i % 12), imsize * (i // 12)))\n",
    "    \n",
    "    grid_image.save(f\"../figs/{model_name_plus_suffix}_{len(all_recons)}recons_{name}_{mode}_{criteria}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27c839fb-583d-4333-9f9b-b5be3369285f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-ckadirt/mindeye/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name_plus_suffix = model_name.replace(\"subj01\", f\"subj{[str(s) for s in subjects]}\")\n",
    "\n",
    "save_plot(all_images[:12], all_recons_best, \"best_multisubject\")\n",
    "save_plot(all_images[:12], all_recons_median, \"median_multisubject\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a6ae46-3de9-4abb-ae2d-c8998d83d6f2",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b794c2d7-ebba-4993-a09d-ffb314cb30e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Create a dictionary to store variable names and their corresponding values\n",
    "# import pandas as pd\n",
    "# data = {\n",
    "#     \"Metric\": [\"PixCorr\", \"SSIM\", \"AlexNet(2)\", \"AlexNet(5)\", \"InceptionV3\", \"CLIP\", \"EffNet-B\", \"SwAV\", \"FwdRetrieval\", \"BwdRetrieval\",\n",
    "#                \"Brain Corr. nsd_general\", \"Brain Corr. V1\", \"Brain Corr. V2\", \"Brain Corr. V3\", \"Brain Corr. V4\",  \"Brain Corr. higher_vis\"],\n",
    "#     \"Value\": [pixcorr, ssim, alexnet2, alexnet5, inception, clip_, effnet, swav, percent_correct_fwd, percent_correct_bwd, \n",
    "#               region_brain_correlations[\"nsd_general\"], region_brain_correlations[\"V1\"], region_brain_correlations[\"V2\"], region_brain_correlations[\"V3\"], region_brain_correlations[\"V4\"], region_brain_correlations[\"higher_vis\"]]}\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "# print(model_name_plus_suffix)\n",
    "# print(df.to_string(index=False))\n",
    "# print(df[\"Value\"].to_string(index=False))\n",
    "\n",
    "# # save table to txt file\n",
    "# os.makedirs('tables/',exist_ok=True)\n",
    "# df[\"Value\"].to_csv(f'tables/{model_name_plus_suffix}.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df1bf004-2128-45fc-9de5-c01c1c187721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# every_to_plot_0 = None\n",
    "# for i in ms_all_recons:\n",
    "#     set_samples = i[1] # shape 10, 3, 512, 512\n",
    "#     if every_to_plot_0 is None:\n",
    "#         every_to_plot_0 = set_samples\n",
    "#     else:\n",
    "#         every_to_plot_0 = torch.cat((every_to_plot_0, set_samples), 0)\n",
    "\n",
    "# print(every_to_plot_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9067132a-d5cb-4d5a-a531-c0bc4f28e923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_image_metrics['Brain Corr. higher_vis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33266038-7f11-491b-8d9c-bc9274190db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_image_metrics['Brain Corr. higher_vis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcf78ae6-1f84-47e8-9a1b-6f2fbf9291fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import torch\n",
    "\n",
    "# iiii = 4\n",
    "\n",
    "# every_to_plot_0 = None\n",
    "# for i in ms_all_recons:\n",
    "#     set_samples = i[iiii]  # shape 10, 3, 512, 512\n",
    "#     if every_to_plot_0 is None:\n",
    "#         every_to_plot_0 = set_samples\n",
    "#     else:\n",
    "#         every_to_plot_0 = torch.cat((every_to_plot_0, set_samples), 0)\n",
    "\n",
    "# print(every_to_plot_0.shape)\n",
    "\n",
    "# current_image_metrics = all_metrics[all_metrics[\"index_image\"] == iiii].reset_index()\n",
    "# scores_values = current_image_metrics['PixCorr'].values  # Get the score values\n",
    "\n",
    "# # Plot all images in a grid with dynamic columns and annotate with score values\n",
    "# def plot_images(images, scores, num_rows=10):\n",
    "#     num_images = len(images)\n",
    "#     num_cols = (num_images + num_rows - 1) // num_rows  # Calculate the number of columns\n",
    "\n",
    "#     fig, axs = plt.subplots(num_rows, num_cols, figsize=(num_cols * 2, num_rows * 2))\n",
    "#     axs = axs.flatten()  # Flatten the axes array for easier indexing\n",
    "\n",
    "#     for idx, image in enumerate(images):\n",
    "#         axs[idx].imshow(image.permute(1, 2, 0))  # Plot image\n",
    "#         axs[idx].axis('off')  # Remove axis\n",
    "#         axs[idx].set_title(f\"Score: {scores[idx]:.2f}\", fontsize=8)  # Add score as the title\n",
    "\n",
    "#     # Hide any unused subplots\n",
    "#     for i in range(len(images), len(axs)):\n",
    "#         axs[i].axis('off')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # Example usage\n",
    "# plot_images(every_to_plot_0.cpu().float(), scores_values, num_rows=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea33429f-f020-47c1-b236-8b6bbe69b818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_pixcorr = transforms.Compose([\n",
    "#     transforms.Resize(425, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "# ])\n",
    "\n",
    "# def get_pix_corr(all_images, all_recons):\n",
    "\n",
    "    \n",
    "#     # Flatten images while keeping the batch dimension\n",
    "#     all_images_flattened = preprocess_pixcorr(all_images).reshape(len(all_images), -1).cpu()\n",
    "#     all_recons_flattened = preprocess_pixcorr(all_recons).view(len(all_recons), -1).cpu()\n",
    "    \n",
    "#     print(all_images_flattened.shape)\n",
    "#     print(all_recons_flattened.shape)\n",
    "    \n",
    "#     corrsum = 0\n",
    "#     for i in tqdm(range(len(all_images))):\n",
    "#         corrsum += np.corrcoef(all_images_flattened[i], all_recons_flattened[i])[0][1]\n",
    "#     corrmean = corrsum / len(all_images)\n",
    "    \n",
    "#     pixcorr = corrmean\n",
    "#     print(pixcorr)\n",
    "#     return pixcorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f1867a5-5ca0-4b33-b540-23a2e110aed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_pix_corr(all_images[4],every_to_plot_0[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
