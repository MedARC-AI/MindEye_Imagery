{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "import gc\n",
    "from itertools import combinations, product\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "sys.path.append('generative_models/')\n",
    "import sgm\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder # bigG embedder\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc5d2e32-6027-4a19-bef4-5ca068db35bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK  0\n"
     ]
    }
   ],
   "source": [
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "data_type = torch.float16 # change depending on your mixed_precision\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: num_devices = 1\n",
    "\n",
    "# First use \"accelerate config\" in terminal and setup using deepspeed stage 2 with CPU offloading!\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\")\n",
    "if utils.is_interactive(): # set batch size here if using interactive notebook instead of submitting job\n",
    "    global_batch_size = batch_size = 21\n",
    "else:\n",
    "    global_batch_size = os.environ[\"GLOBAL_BATCH_SIZE\"]\n",
    "    batch_size = int(os.environ[\"GLOBAL_BATCH_SIZE\"]) // num_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b767ab6f-d4a9-47a5-b3bf-f56bf6760c0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID of this process = 65580\n",
      "device: cuda\n",
      "Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1 data_type = torch.float16\n"
     ]
    }
   ],
   "source": [
    "print(\"PID of this process =\",os.getpid())\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0 or not distributed: num_devices = 1\n",
    "num_workers = num_devices\n",
    "print(accelerator.state)\n",
    "\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)\n",
    "print = accelerator.print # only print if local_rank=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018b82b-c054-4463-9527-4b0c2a75bda6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b61fec7-72a0-4b67-86da-1375f1d9fbd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: multisubject_subj01_hypatia_contrastive\n",
      "--data_path=../dataset                     --cache_dir=../cache                     --model_name=multisubject_subj01_hypatia_contrastive                     --multi_subject --subj=1 --batch_size=21 --num_sessions=40                     --hidden_dim=1024 --clip_scale=1.                     --blur_scale=.5                      --no-blurry_recon                     --seq_past=0 --seq_future=0                     --use_prior --prior_scale=30                     --n_blocks=4 --max_lr=3e-4 --mixup_pct=.33 --num_epochs=150 --no-use_image_aug                     --ckpt_interval=1 --ckpt_saving --wandb_log \n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name = \"multisubject_subj01_hypatia_contrastive\"\n",
    "    print(\"model_name:\", model_name)\n",
    "    \n",
    "    # global_batch_size and batch_size should already be defined in the 2nd cell block\n",
    "    jupyter_args = f\"--data_path=../dataset \\\n",
    "                    --cache_dir=../cache \\\n",
    "                    --model_name={model_name} \\\n",
    "                    --multi_subject --subj=1 --batch_size={batch_size} --num_sessions=40 \\\n",
    "                    --hidden_dim=1024 --clip_scale=1. \\\n",
    "                    --blur_scale=.5  \\\n",
    "                    --no-blurry_recon \\\n",
    "                    --seq_past=0 --seq_future=0 \\\n",
    "                    --use_prior --prior_scale=30 \\\n",
    "                    --n_blocks=4 --max_lr=3e-4 --mixup_pct=.33 --num_epochs=150 --no-use_image_aug \\\n",
    "                    --ckpt_interval=1 --ckpt_saving --wandb_log \"\n",
    "\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2028bdf0-2f41-46d9-b6e7-86b870dbf16c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj_list [2 3 4 5 6 7 8] num_sessions 40\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where misc. files downloaded from huggingface are stored. Defaults to current src directory.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multisubject_ckpt\", type=str, default=None,\n",
    "    help=\"Path to pre-trained multisubject model to finetune a single subject from. multisubject must be False.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_sessions\", type=int, default=1,\n",
    "    help=\"Number of training sessions to include\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_prior\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to train diffusion prior (True) or just rely on retrieval part of the pipeline (False)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--visualize_prior\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"output visualizations from unCLIP every ckpt_interval (requires much more memory!)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=16,\n",
    "    help=\"Batch size can be increased by 10x if only training retreival submodule and not diffusion prior\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--resume_from_ckpt\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"if not using wandb and want to resume from a ckpt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mixup_pct\",type=float,default=.33,\n",
    "    help=\"proportion of way through training when to switch from BiMixCo to SoftCLIP\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to output blurry reconstructions\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blur_scale\",type=float,default=.5,\n",
    "    help=\"multiply loss from blurry recons by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_scale\",type=float,default=1.,\n",
    "    help=\"multiply contrastive loss by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--contrastive_scale\",type=float,default=1.,\n",
    "    help=\"multiply trial-wise contrastive loss by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--prior_scale\",type=float,default=30,\n",
    "    help=\"multiply diffusion prior loss by this\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_image_aug\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to use image augmentation\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=150,\n",
    "    help=\"number of epochs of training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multi_subject\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--new_test\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=1024,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_past\",type=int,default=0,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_future\",type=int,default=0,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",type=str,default='cycle',choices=['cycle','linear'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=5,\n",
    "    help=\"save backup ckpt and reconstruct every x epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "if not os.path.exists(outdir) and ckpt_saving:\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "    \n",
    "if use_image_aug or blurry_recon:\n",
    "    import kornia\n",
    "    from kornia.augmentation.container import AugmentationSequential\n",
    "if use_image_aug:\n",
    "    img_augment = AugmentationSequential(\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1, p=0.3),\n",
    "        same_on_batch=False,\n",
    "        data_keys=[\"input\"],\n",
    "    )\n",
    "    \n",
    "if multi_subject:\n",
    "    subj_list = np.arange(1,9)\n",
    "    subj_list = subj_list[subj_list != subj]\n",
    "else:\n",
    "    subj_list = [subj]\n",
    "\n",
    "print(\"subj_list\", subj_list, \"num_sessions\", num_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prep data, models, and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c023f24-5233-4a15-a2f5-78487b3a8546",
   "metadata": {},
   "source": [
    "### Creating wds dataloader, preload betas and all 73k possible images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aefe7c27-ab39-4b2c-90f4-480f4087b7ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dividing batch size by subj_list, which will then be concatenated across subj during training...\n",
      "batch_size = 1 num_iterations_per_epoch = 4285 num_samples_per_epoch = 30000\n"
     ]
    }
   ],
   "source": [
    "def my_split_by_node(urls): return urls\n",
    "num_voxels_list = []\n",
    "\n",
    "if multi_subject:\n",
    "    nsessions_allsubj=np.array([40, 40, 32, 30, 40, 32, 40, 30])\n",
    "    num_samples_per_epoch = (750*40) // num_devices \n",
    "else:\n",
    "    num_samples_per_epoch = (750*num_sessions) // num_devices \n",
    "\n",
    "print(\"dividing batch size by subj_list, which will then be concatenated across subj during training...\") \n",
    "batch_size = batch_size // len(subj_list)\n",
    "# batch_size = batch_size // 3 # divide by 3 because we need to run trial repetitions through as well\n",
    "\n",
    "num_iterations_per_epoch = num_samples_per_epoch // (batch_size*len(subj_list))\n",
    "\n",
    "print(\"batch_size =\", batch_size, \"num_iterations_per_epoch =\",num_iterations_per_epoch, \"num_samples_per_epoch =\",num_samples_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81084834-035f-4465-ad59-59e6b806a2f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 40 sessions\n",
      "../dataset/wds/subj02/train/{0..39}.tar\n",
      "num_voxels for subj02: 14278\n",
      "Training with 40 sessions\n",
      "../dataset/wds/subj03/train/{0..31}.tar\n",
      "num_voxels for subj03: 15226\n",
      "Training with 40 sessions\n",
      "../dataset/wds/subj04/train/{0..29}.tar\n",
      "num_voxels for subj04: 13153\n",
      "Training with 40 sessions\n",
      "../dataset/wds/subj05/train/{0..39}.tar\n",
      "num_voxels for subj05: 13039\n",
      "Training with 40 sessions\n",
      "../dataset/wds/subj06/train/{0..31}.tar\n",
      "num_voxels for subj06: 17907\n",
      "Training with 40 sessions\n",
      "../dataset/wds/subj07/train/{0..39}.tar\n",
      "num_voxels for subj07: 12682\n",
      "Training with 40 sessions\n",
      "../dataset/wds/subj08/train/{0..29}.tar\n",
      "num_voxels for subj08: 14386\n",
      "Loaded all subj train dls and betas!\n",
      "\n",
      "../dataset/wds/subj02/new_test/0.tar\n",
      "Loaded test dl for subj2!\n",
      "\n",
      "currently using 1 seq_len (chose 0 past behav and 0 future behav)\n"
     ]
    }
   ],
   "source": [
    "train_data = {}\n",
    "train_dl = {}\n",
    "num_voxels = {}\n",
    "voxels = {}\n",
    "for s in subj_list:\n",
    "    print(f\"Training with {num_sessions} sessions\")\n",
    "    if multi_subject:\n",
    "        train_url = f\"{data_path}/wds/subj0{s}/train/\" + \"{0..\" + f\"{nsessions_allsubj[s-1]-1}\" + \"}.tar\"\n",
    "    else:\n",
    "        train_url = f\"{data_path}/wds/subj0{s}/train/\" + \"{0..\" + f\"{num_sessions-1}\" + \"}.tar\"\n",
    "    print(train_url)\n",
    "    \n",
    "    train_data[f'subj0{s}'] = wds.WebDataset(train_url,resampled=True,nodesplitter=my_split_by_node)\\\n",
    "                        .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                        .decode(\"torch\")\\\n",
    "                        .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                        .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "    train_dl[f'subj0{s}'] = torch.utils.data.DataLoader(train_data[f'subj0{s}'], batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "\n",
    "    f = h5py.File(f'{data_path}/betas_all_subj0{s}_fp32_renorm.hdf5', 'r')\n",
    "    betas = f['betas'][:]\n",
    "    betas = torch.Tensor(betas).to(\"cpu\").to(data_type)\n",
    "    num_voxels_list.append(betas[0].shape[-1])\n",
    "    num_voxels[f'subj0{s}'] = betas[0].shape[-1]\n",
    "    voxels[f'subj0{s}'] = betas\n",
    "    print(f\"num_voxels for subj0{s}: {num_voxels[f'subj0{s}']}\")\n",
    "\n",
    "print(\"Loaded all subj train dls and betas!\\n\")\n",
    "\n",
    "# Validate only on one subject\n",
    "if multi_subject: \n",
    "    subj = subj_list[0] # cant validate on the actual held out person so picking first in subj_list\n",
    "if not new_test: # using old test set from before full dataset released (used in original MindEye paper)\n",
    "    if subj==3:\n",
    "        num_test=2113\n",
    "    elif subj==4:\n",
    "        num_test=1985\n",
    "    elif subj==6:\n",
    "        num_test=2113\n",
    "    elif subj==8:\n",
    "        num_test=1985\n",
    "    else:\n",
    "        num_test=2770\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/test/\" + \"0.tar\"\n",
    "elif new_test: # using larger test set from after full dataset released\n",
    "    if subj==3:\n",
    "        num_test=2371\n",
    "    elif subj==4:\n",
    "        num_test=2188\n",
    "    elif subj==6:\n",
    "        num_test=2371\n",
    "    elif subj==8:\n",
    "        num_test=2188\n",
    "    else:\n",
    "        num_test=3000\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/new_test/\" + \"0.tar\"\n",
    "print(test_url)\n",
    "test_data = wds.WebDataset(test_url,resampled=False,nodesplitter=my_split_by_node)\\\n",
    "                    .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                    .decode(\"torch\")\\\n",
    "                    .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                    .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=True, pin_memory=True)\n",
    "print(f\"Loaded test dl for subj{subj}!\\n\")\n",
    "\n",
    "seq_len = seq_past + 1 + seq_future\n",
    "print(f\"currently using {seq_len} seq_len (chose {seq_past} past behav and {seq_future} future behav)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c13b4b84-094c-4b5b-bace-26c155aa6181",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all 73k possible NSD images to cpu! torch.Size([73000, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Load 73k NSD images\n",
    "f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images'][:] # if you go OOM you can remove the [:] so it isnt preloaded to cpu! (will require a few edits elsewhere tho)\n",
    "images = torch.Tensor(images).to(\"cpu\").to(data_type)\n",
    "print(\"Loaded all 73k possible NSD images to cpu!\", images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec4517-dbdf-4ece-98f6-4714d5de4e15",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d6160e-1ee8-4da7-a755-9dbb452a6fa5",
   "metadata": {},
   "source": [
    "### CLIP image embeddings  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0420dc0-199e-4c1a-857d-b1747058b467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "#     arch=\"ViT-bigG-14\",\n",
    "#     version=\"laion2b_s39b_b160k\",\n",
    "#     output_tokens=True,\n",
    "#     only_tokens=True,\n",
    "# )\n",
    "# clip_img_embedder.to(device)\n",
    "\n",
    "# clip_seq_dim = 256\n",
    "# clip_emb_dim = 1664\n",
    "\n",
    "import clip\n",
    "clip_model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                         std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "])\n",
    "\n",
    "def clip_img_embedder(image):\n",
    "    preproc_img = preprocess(image)\n",
    "    return clip_model.encode_image(preproc_img)\n",
    "clip_seq_dim = 1\n",
    "clip_emb_dim = 768"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b79bd38-6990-4504-8d45-4a68d57d8885",
   "metadata": {},
   "source": [
    "### SD VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01baff79-8114-482b-b115-6f05aa8ad691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if blurry_recon:\n",
    "    from diffusers import AutoencoderKL    \n",
    "    autoenc = AutoencoderKL(\n",
    "        down_block_types=['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'],\n",
    "        up_block_types=['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'],\n",
    "        block_out_channels=[128, 256, 512, 512],\n",
    "        layers_per_block=2,\n",
    "        sample_size=256,\n",
    "    )\n",
    "    ckpt = torch.load(f'{cache_dir}/sd_image_var_autoenc.pth')\n",
    "    autoenc.load_state_dict(ckpt)\n",
    "    \n",
    "    autoenc.eval()\n",
    "    autoenc.requires_grad_(False)\n",
    "    autoenc.to(device)\n",
    "    utils.count_params(autoenc)\n",
    "    \n",
    "    from autoencoder.convnext import ConvnextXL\n",
    "    cnx = ConvnextXL(f'{cache_dir}/convnext_xlarge_alpha0.75_fullckpt.pth')\n",
    "    cnx.requires_grad_(False)\n",
    "    cnx.eval()\n",
    "    cnx.to(device)\n",
    "    \n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).to(device).reshape(1,3,1,1)\n",
    "    std = torch.tensor([0.228, 0.224, 0.225]).to(device).reshape(1,3,1,1)\n",
    "    \n",
    "    blur_augs = AugmentationSequential(\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1, p=0.8),\n",
    "        kornia.augmentation.RandomGrayscale(p=0.1),\n",
    "        kornia.augmentation.RandomSolarize(p=0.1),\n",
    "        kornia.augmentation.RandomResizedCrop((224,224), scale=(.9,.9), ratio=(1,1), p=1.0),\n",
    "        data_keys=[\"input\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e5e4a-f697-4b2c-88fc-01f6a54886c0",
   "metadata": {},
   "source": [
    "### MindEye modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c44c271b-173f-472e-b059-a2eda0f4c4c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MindEyeModule()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "model = MindEyeModule()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "038a5d61-4769-40b9-a004-f4e7b5b38bb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "103,094,272 total\n",
      "103,094,272 trainable\n",
      "param counts:\n",
      "103,094,272 total\n",
      "103,094,272 trainable\n",
      "torch.Size([2, 1, 14278]) torch.Size([2, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "class RidgeRegression(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer\n",
    "    def __init__(self, input_sizes, out_features, seq_len): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "            ])\n",
    "    def forward(self, x, subj_idx):\n",
    "        out = torch.cat([self.linears[subj_idx](x[:,seq]).unsqueeze(1) for seq in range(seq_len)], dim=1)\n",
    "        return out\n",
    "        \n",
    "model.ridge = RidgeRegression(num_voxels_list, out_features=hidden_dim, seq_len=seq_len)\n",
    "utils.count_params(model.ridge)\n",
    "utils.count_params(model)\n",
    "\n",
    "# test on subject 1 with fake data\n",
    "b = torch.randn((2,seq_len,num_voxels_list[0]))\n",
    "print(b.shape, model.ridge(b,0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b8de65a-6d3b-4248-bea9-9b6f4d562321",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "453,360,280 total\n",
      "453,360,280 trainable\n",
      "param counts:\n",
      "556,454,552 total\n",
      "556,454,552 trainable\n",
      "b.shape torch.Size([2, 1, 1024])\n",
      "torch.Size([2, 256, 1664]) torch.Size([2, 256, 1664]) torch.Size([1]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "from models import BrainNetwork\n",
    "model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=seq_len, n_blocks=n_blocks,\n",
    "                          clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim, \n",
    "                          blurry_recon=blurry_recon, clip_scale=clip_scale)\n",
    "utils.count_params(model.backbone)\n",
    "utils.count_params(model)\n",
    "\n",
    "# test that the model works on some fake data\n",
    "b = torch.randn((2,seq_len,hidden_dim))\n",
    "print(\"b.shape\",b.shape)\n",
    "\n",
    "backbone_, clip_, blur_ = model.backbone(b)\n",
    "print(backbone_.shape, clip_.shape, blur_[0].shape, blur_[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b397c0d7-52a3-4153-823b-c27d2eb3eeba",
   "metadata": {},
   "source": [
    "### Adding diffusion prior + unCLIP if use_prior=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69965344-9346-4592-9cc5-e537e31d5fce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "259,865,216 total\n",
      "259,865,200 trainable\n",
      "param counts:\n",
      "816,319,768 total\n",
      "816,319,752 trainable\n"
     ]
    }
   ],
   "source": [
    "if use_prior:\n",
    "    from models import *\n",
    "\n",
    "    # setup diffusion prior network\n",
    "    out_dim = clip_emb_dim\n",
    "    depth = 6\n",
    "    dim_head = 52\n",
    "    heads = clip_emb_dim//52 # heads * dim_head = clip_emb_dim\n",
    "    timesteps = 100\n",
    "\n",
    "    prior_network = PriorNetwork(\n",
    "            dim=out_dim,\n",
    "            depth=depth,\n",
    "            dim_head=dim_head,\n",
    "            heads=heads,\n",
    "            causal=False,\n",
    "            num_tokens = clip_seq_dim,\n",
    "            learned_query_mode=\"pos_emb\"\n",
    "        )\n",
    "\n",
    "    model.diffusion_prior = BrainDiffusionPrior(\n",
    "        net=prior_network,\n",
    "        image_embed_dim=out_dim,\n",
    "        condition_on_text_encodings=False,\n",
    "        timesteps=timesteps,\n",
    "        cond_drop_prob=0.2,\n",
    "        image_embed_scale=None,\n",
    "    )\n",
    "    \n",
    "    utils.count_params(model.diffusion_prior)\n",
    "    utils.count_params(model)\n",
    "    \n",
    "    # prep unCLIP\n",
    "    if visualize_prior:\n",
    "        from generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "        from omegaconf import OmegaConf\n",
    "        \n",
    "        config = OmegaConf.load(\"generative_models/configs/unclip6.yaml\")\n",
    "        config = OmegaConf.to_container(config, resolve=True)\n",
    "        unclip_params = config[\"model\"][\"params\"]\n",
    "        network_config = unclip_params[\"network_config\"]\n",
    "        denoiser_config = unclip_params[\"denoiser_config\"]\n",
    "        first_stage_config = unclip_params[\"first_stage_config\"]\n",
    "        conditioner_config = unclip_params[\"conditioner_config\"]\n",
    "        sampler_config = unclip_params[\"sampler_config\"]\n",
    "        scale_factor = unclip_params[\"scale_factor\"]\n",
    "        disable_first_stage_autocast = unclip_params[\"disable_first_stage_autocast\"]\n",
    "        offset_noise_level = unclip_params[\"loss_fn_config\"][\"params\"][\"offset_noise_level\"]\n",
    "\n",
    "        first_stage_config['target'] = 'sgm.models.autoencoder.AutoencoderKL'\n",
    "        sampler_config['params']['num_steps'] = 38\n",
    "\n",
    "        diffusion_engine = DiffusionEngine(network_config=network_config,\n",
    "                               denoiser_config=denoiser_config,\n",
    "                               first_stage_config=first_stage_config,\n",
    "                               conditioner_config=conditioner_config,\n",
    "                               sampler_config=sampler_config,\n",
    "                               scale_factor=scale_factor,\n",
    "                               disable_first_stage_autocast=disable_first_stage_autocast)\n",
    "        # set to inference\n",
    "        diffusion_engine.eval().requires_grad_(False)\n",
    "        diffusion_engine.to(device)\n",
    "\n",
    "        ckpt_path = f'{cache_dir}/unclip6_epoch0_step110000.ckpt'\n",
    "        ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "        diffusion_engine.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "        image = images[:1].to(device)\n",
    "        batch={\"jpg\": image,\n",
    "              \"original_size_as_tuple\": torch.ones(image.shape[0], 2).to(device) * 768,\n",
    "              \"crop_coords_top_left\": torch.zeros(image.shape[0], 2).to(device)}\n",
    "        out = diffusion_engine.conditioner(batch)\n",
    "        vector_suffix = out[\"vector\"].to(device)\n",
    "        print(\"vector_suffix\", vector_suffix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec25271a-2209-400c-8026-df3b8ddc1eef",
   "metadata": {},
   "source": [
    "### Setup optimizer / lr / ckpt saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e14d0482-dc42-43b9-9ce1-953c32f2c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps 642750\n",
      "\n",
      "Done with model preparations!\n",
      "param counts:\n",
      "816,319,768 total\n",
      "816,319,752 trainable\n"
     ]
    }
   ],
   "source": [
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.ridge.named_parameters()], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "if use_prior:\n",
    "    opt_grouped_parameters.extend([\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ])\n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "\n",
    "if lr_scheduler_type == 'linear':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        total_iters=int(np.floor(num_epochs*num_iterations_per_epoch)),\n",
    "        last_epoch=-1\n",
    "    )\n",
    "elif lr_scheduler_type == 'cycle':\n",
    "    total_steps=int(np.floor(num_epochs*num_iterations_per_epoch))\n",
    "    print(\"total_steps\", total_steps)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr,\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n",
    "    \n",
    "def save_ckpt(tag):\n",
    "    ckpt_path = outdir+f'/{tag}.pth'\n",
    "    if accelerator.is_main_process:\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': unwrapped_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'train_losses': losses,\n",
    "            'test_losses': test_losses,\n",
    "            'lrs': lrs,\n",
    "            }, ckpt_path)\n",
    "    print(f\"\\n---saved {outdir}/{tag} ckpt!---\\n\")\n",
    "\n",
    "def load_ckpt(tag,load_lr=True,load_optimizer=True,load_epoch=True,strict=True,outdir=outdir,multisubj_loading=False): \n",
    "    print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "    checkpoint = torch.load(outdir+'/last.pth', map_location='cpu')\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    if multisubj_loading: # remove incompatible ridge layer that will otherwise error\n",
    "        state_dict.pop('ridge.linears.0.weight',None)\n",
    "    model.load_state_dict(state_dict, strict=strict)\n",
    "    if load_epoch:\n",
    "        globals()[\"epoch\"] = checkpoint['epoch']\n",
    "        print(\"Epoch\",epoch)\n",
    "    if load_optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if load_lr:\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    del checkpoint\n",
    "\n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f458b-35b8-49f2-b6db-80296cece730",
   "metadata": {},
   "source": [
    "# Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a25a662-daa8-4de9-9233-8364800fcb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb mindeye run multisubject_subj01_hypatia_contrastive\n",
      "wandb_config:\n",
      " {'model_name': 'multisubject_subj01_hypatia_contrastive', 'global_batch_size': 21, 'batch_size': 1, 'num_epochs': 150, 'num_sessions': 40, 'num_params': 816319752, 'clip_scale': 1.0, 'prior_scale': 30.0, 'blur_scale': 0.5, 'use_image_aug': False, 'max_lr': 0.0003, 'mixup_pct': 0.33, 'num_samples_per_epoch': 30000, 'num_test': 3000, 'ckpt_interval': 1, 'ckpt_saving': True, 'seed': 42, 'distributed': False, 'num_devices': 1, 'world_size': 1, 'train_url': '../dataset/wds/subj08/train/{0..29}.tar', 'test_url': '../dataset/wds/subj02/new_test/0.tar'}\n",
      "wandb_id: multisubject_subj01_hypatia_contrastive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mreesekneeland\u001b[0m (\u001b[33mbrain_decoder\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/export/raid1/home/kneel027/MindEye_Imagery/src/wandb/run-20240424_212058-multisubject_subj01_hypatia_contrastive</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/brain_decoder/mindeye/runs/multisubject_subj01_hypatia_contrastive' target=\"_blank\">multisubject_subj01_hypatia_contrastive</a></strong> to <a href='https://wandb.ai/brain_decoder/mindeye' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/brain_decoder/mindeye' target=\"_blank\">https://wandb.ai/brain_decoder/mindeye</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/brain_decoder/mindeye/runs/multisubject_subj01_hypatia_contrastive' target=\"_blank\">https://wandb.ai/brain_decoder/mindeye/runs/multisubject_subj01_hypatia_contrastive</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if local_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'mindeye'\n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_sessions\": num_sessions,\n",
    "      \"num_params\": num_params,\n",
    "      \"clip_scale\": clip_scale,\n",
    "      \"contrastive_scale\" : contrastive_scale,\n",
    "      \"prior_scale\": prior_scale,\n",
    "      \"blur_scale\": blur_scale,\n",
    "      \"use_image_aug\": use_image_aug,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"mixup_pct\": mixup_pct,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "      \"num_test\": num_test,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_url\": train_url,\n",
    "      \"test_url\": test_url,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        id=model_name,\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5690151-2131-4918-b750-e869cbd1a8a8",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12de6387-6e18-4e4b-b5ce-a847d625330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "losses, test_losses, lrs = [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "607a7c7b-fe5e-41a4-80bf-d2814b3a57cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load multisubject stage1 ckpt if set\n",
    "if multisubject_ckpt is not None and not resume_from_ckpt:\n",
    "    load_ckpt(\"last\",outdir=multisubject_ckpt,load_lr=False,load_optimizer=False,load_epoch=False,strict=False,multisubj_loading=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5453c316-0cb0-4bee-8585-f44dff746e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---loading /export/raid1/home/kneel027/MindEye_Imagery/train_logs/multisubject_subj01_hypatia_contrastive/last.pth ckpt---\n",
      "\n",
      "Epoch 0\n"
     ]
    }
   ],
   "source": [
    "# load saved ckpt model weights into current model\n",
    "if resume_from_ckpt:\n",
    "    load_ckpt(\"last\",load_lr=True,load_optimizer=True,load_epoch=True)\n",
    "# elif wandb_log:\n",
    "#     if wandb.run.resumed:\n",
    "#         load_ckpt(\"last\",load_lr=True,load_optimizer=True,load_epoch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99f09f76-4481-4133-b09a-a22b10dbc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dls = [train_dl[f'subj0{s}'] for s in subj_list]\n",
    "\n",
    "model, optimizer, *train_dls, lr_scheduler = accelerator.prepare(model, optimizer, *train_dls, lr_scheduler)\n",
    "# leaving out test_dl since we will only have local_rank 0 device do evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60be0d5f-3e94-4612-9373-61b53d836393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multisubject_subj01_hypatia_contrastive starting with epoch 0 / 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | 0/150 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([3, 4, 5, 4, 5, 0, 5], device='cuda:0')\n",
      "negative indices: tensor([2, 4, 6, 4, 1, 6, 2], device='cuda:0')\n",
      "negative indices: tensor([2, 4, 5, 4, 3, 6, 2], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([3, 5, 1, 2, 5, 4, 2], device='cuda:0')\n",
      "negative indices: tensor([5, 6, 6, 5, 5, 4, 1], device='cuda:0')\n",
      "negative indices: tensor([4, 6, 1, 5, 1, 1, 5], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([5, 5, 1, 5, 6, 2, 4], device='cuda:0')\n",
      "negative indices: tensor([3, 4, 5, 0, 2, 1, 4], device='cuda:0')\n",
      "negative indices: tensor([5, 5, 5, 0, 6, 4, 4], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([6, 6, 6, 5, 0, 2, 2], device='cuda:0')\n",
      "negative indices: tensor([4, 4, 0, 5, 2, 0, 0], device='cuda:0')\n",
      "negative indices: tensor([6, 6, 6, 5, 2, 0, 0], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([5, 3, 5, 1, 1, 0, 1], device='cuda:0')\n",
      "negative indices: tensor([5, 5, 5, 1, 0, 2, 1], device='cuda:0')\n",
      "negative indices: tensor([5, 6, 5, 1, 0, 2, 1], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([1, 4, 0, 6, 6, 2, 4], device='cuda:0')\n",
      "negative indices: tensor([4, 4, 5, 6, 1, 3, 0], device='cuda:0')\n",
      "negative indices: tensor([2, 4, 5, 4, 1, 6, 5], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([4, 6, 4, 4, 6, 6, 0], device='cuda:0')\n",
      "negative indices: tensor([4, 6, 4, 6, 2, 6, 0], device='cuda:0')\n",
      "negative indices: tensor([4, 0, 4, 4, 2, 6, 3], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([3, 3, 5, 0, 0, 2, 4], device='cuda:0')\n",
      "negative indices: tensor([4, 4, 5, 6, 0, 2, 2], device='cuda:0')\n",
      "negative indices: tensor([4, 6, 5, 6, 2, 2, 4], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([2, 6, 4, 5, 6, 0, 4], device='cuda:0')\n",
      "negative indices: tensor([6, 4, 1, 5, 2, 3, 1], device='cuda:0')\n",
      "negative indices: tensor([6, 0, 6, 5, 2, 1, 1], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([2, 6, 5, 6, 1, 2, 3], device='cuda:0')\n",
      "negative indices: tensor([5, 6, 0, 6, 5, 0, 2], device='cuda:0')\n",
      "negative indices: tensor([5, 4, 0, 6, 6, 0, 3], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([6, 5, 6, 2, 1, 0, 0], device='cuda:0')\n",
      "negative indices: tensor([5, 3, 0, 1, 5, 2, 0], device='cuda:0')\n",
      "negative indices: tensor([5, 3, 0, 2, 0, 6, 2], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([2, 5, 1, 1, 1, 1, 4], device='cuda:0')\n",
      "negative indices: tensor([5, 5, 5, 1, 5, 2, 0], device='cuda:0')\n",
      "negative indices: tensor([4, 3, 0, 1, 6, 2, 4], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([5, 2, 3, 4, 6, 4, 5], device='cuda:0')\n",
      "negative indices: tensor([6, 2, 1, 4, 3, 6, 5], device='cuda:0')\n",
      "negative indices: tensor([5, 4, 6, 4, 5, 0, 5], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([6, 0, 5, 4, 1, 2, 2], device='cuda:0')\n",
      "negative indices: tensor([6, 0, 5, 0, 1, 2, 2], device='cuda:0')\n",
      "negative indices: tensor([6, 4, 0, 0, 0, 2, 2], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([2, 2, 6, 1, 5, 3, 2], device='cuda:0')\n",
      "negative indices: tensor([2, 2, 0, 1, 2, 3, 5], device='cuda:0')\n",
      "negative indices: tensor([2, 4, 0, 1, 1, 3, 0], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([5, 5, 4, 6, 5, 1, 4], device='cuda:0')\n",
      "negative indices: tensor([5, 0, 1, 2, 0, 0, 5], device='cuda:0')\n",
      "negative indices: tensor([2, 0, 0, 6, 1, 0, 3], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([2, 4, 4, 0, 1, 0, 2], device='cuda:0')\n",
      "negative indices: tensor([6, 4, 4, 6, 1, 6, 2], device='cuda:0')\n",
      "negative indices: tensor([5, 4, 6, 6, 1, 6, 5], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([3, 4, 0, 0, 3, 3, 1], device='cuda:0')\n",
      "negative indices: tensor([3, 6, 0, 0, 1, 0, 0], device='cuda:0')\n",
      "negative indices: tensor([1, 4, 6, 4, 6, 0, 1], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([6, 0, 5, 4, 1, 1, 2], device='cuda:0')\n",
      "negative indices: tensor([4, 2, 5, 6, 2, 2, 3], device='cuda:0')\n",
      "negative indices: tensor([2, 2, 5, 6, 6, 2, 5], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([3, 5, 4, 0, 2, 4, 4], device='cuda:0')\n",
      "negative indices: tensor([1, 6, 1, 6, 2, 2, 2], device='cuda:0')\n",
      "negative indices: tensor([3, 4, 6, 1, 2, 2, 2], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([1, 4, 4, 1, 5, 2, 5], device='cuda:0')\n",
      "negative indices: tensor([5, 4, 1, 6, 5, 6, 5], device='cuda:0')\n",
      "negative indices: tensor([1, 0, 6, 1, 1, 6, 5], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([6, 4, 4, 4, 0, 3, 3], device='cuda:0')\n",
      "negative indices: tensor([5, 4, 6, 4, 3, 0, 3], device='cuda:0')\n",
      "negative indices: tensor([3, 6, 6, 0, 3, 0, 0], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([5, 4, 4, 4, 1, 0, 3], device='cuda:0')\n",
      "negative indices: tensor([4, 6, 0, 2, 1, 0, 3], device='cuda:0')\n",
      "negative indices: tensor([2, 4, 5, 2, 2, 0, 2], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([1, 2, 5, 4, 3, 1, 4], device='cuda:0')\n",
      "negative indices: tensor([1, 2, 5, 4, 3, 0, 4], device='cuda:0')\n",
      "negative indices: tensor([1, 0, 3, 2, 2, 6, 1], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([6, 0, 4, 4, 2, 2, 5], device='cuda:0')\n",
      "negative indices: tensor([5, 5, 3, 1, 3, 2, 5], device='cuda:0')\n",
      "negative indices: tensor([5, 5, 6, 1, 1, 2, 5], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([3, 2, 5, 0, 6, 4, 4], device='cuda:0')\n",
      "negative indices: tensor([6, 2, 3, 0, 6, 2, 4], device='cuda:0')\n",
      "negative indices: tensor([6, 2, 3, 0, 6, 2, 4], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([1, 2, 5, 1, 2, 2, 0], device='cuda:0')\n",
      "negative indices: tensor([4, 6, 6, 1, 6, 2, 0], device='cuda:0')\n",
      "negative indices: tensor([4, 3, 6, 1, 3, 2, 5], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([2, 2, 5, 6, 1, 2, 3], device='cuda:0')\n",
      "negative indices: tensor([5, 2, 1, 1, 5, 2, 3], device='cuda:0')\n",
      "negative indices: tensor([2, 4, 5, 1, 3, 2, 3], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([5, 6, 6, 1, 1, 0, 3], device='cuda:0')\n",
      "negative indices: tensor([5, 2, 1, 4, 5, 0, 3], device='cuda:0')\n",
      "negative indices: tensor([5, 4, 0, 1, 5, 2, 2], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([1, 2, 5, 4, 3, 0, 1], device='cuda:0')\n",
      "negative indices: tensor([6, 0, 5, 1, 1, 6, 1], device='cuda:0')\n",
      "negative indices: tensor([6, 3, 4, 1, 1, 6, 3], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([5, 3, 3, 2, 3, 0, 5], device='cuda:0')\n",
      "negative indices: tensor([5, 3, 4, 5, 3, 0, 5], device='cuda:0')\n",
      "negative indices: tensor([5, 4, 4, 4, 1, 0, 0], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([4, 0, 5, 4, 6, 2, 1], device='cuda:0')\n",
      "negative indices: tensor([3, 4, 5, 0, 3, 2, 4], device='cuda:0')\n",
      "negative indices: tensor([5, 4, 5, 0, 3, 2, 5], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([4, 6, 3, 2, 6, 0, 5], device='cuda:0')\n",
      "negative indices: tensor([6, 4, 3, 1, 3, 6, 5], device='cuda:0')\n",
      "negative indices: tensor([6, 4, 3, 2, 2, 2, 1], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([1, 3, 6, 1, 1, 0, 4], device='cuda:0')\n",
      "negative indices: tensor([1, 0, 6, 6, 0, 4, 4], device='cuda:0')\n",
      "negative indices: tensor([1, 0, 6, 2, 5, 4, 2], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([3, 5, 4, 0, 1, 1, 1], device='cuda:0')\n",
      "negative indices: tensor([4, 5, 0, 4, 5, 1, 5], device='cuda:0')\n",
      "negative indices: tensor([2, 5, 0, 0, 3, 1, 5], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([4, 5, 1, 5, 1, 1, 1], device='cuda:0')\n",
      "negative indices: tensor([4, 2, 5, 6, 1, 2, 2], device='cuda:0')\n",
      "negative indices: tensor([6, 2, 4, 5, 6, 0, 5], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([4, 5, 1, 6, 1, 6, 4], device='cuda:0')\n",
      "negative indices: tensor([4, 2, 5, 6, 1, 6, 4], device='cuda:0')\n",
      "negative indices: tensor([4, 4, 5, 4, 1, 4, 1], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([3, 3, 0, 0, 3, 1, 1], device='cuda:0')\n",
      "negative indices: tensor([3, 4, 0, 0, 3, 1, 5], device='cuda:0')\n",
      "negative indices: tensor([3, 5, 1, 0, 2, 6, 5], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([5, 4, 4, 1, 5, 0, 5], device='cuda:0')\n",
      "negative indices: tensor([1, 4, 4, 6, 5, 1, 0], device='cuda:0')\n",
      "negative indices: tensor([5, 3, 4, 1, 3, 0, 5], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([3, 4, 0, 1, 6, 0, 1], device='cuda:0')\n",
      "negative indices: tensor([3, 3, 5, 0, 6, 6, 3], device='cuda:0')\n",
      "negative indices: tensor([5, 3, 0, 0, 2, 3, 3], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([1, 6, 1, 6, 6, 4, 5], device='cuda:0')\n",
      "negative indices: tensor([6, 3, 0, 6, 5, 4, 5], device='cuda:0')\n",
      "negative indices: tensor([6, 0, 5, 5, 5, 4, 4], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([6, 4, 0, 6, 6, 3, 5], device='cuda:0')\n",
      "negative indices: tensor([4, 0, 6, 4, 5, 0, 3], device='cuda:0')\n",
      "negative indices: tensor([3, 0, 6, 5, 1, 0, 3], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([5, 3, 1, 0, 0, 4, 2], device='cuda:0')\n",
      "negative indices: tensor([5, 3, 6, 5, 0, 4, 2], device='cuda:0')\n",
      "negative indices: tensor([5, 5, 0, 5, 1, 0, 3], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([4, 5, 6, 2, 5, 0, 5], device='cuda:0')\n",
      "negative indices: tensor([4, 5, 0, 6, 0, 0, 0], device='cuda:0')\n",
      "negative indices: tensor([5, 5, 6, 4, 3, 0, 1], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([2, 6, 5, 5, 1, 1, 1], device='cuda:0')\n",
      "negative indices: tensor([2, 0, 0, 5, 1, 1, 4], device='cuda:0')\n",
      "negative indices: tensor([2, 4, 3, 4, 6, 3, 1], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([1, 2, 4, 4, 2, 0, 1], device='cuda:0')\n",
      "negative indices: tensor([5, 2, 3, 4, 1, 4, 2], device='cuda:0')\n",
      "negative indices: tensor([5, 6, 0, 5, 5, 4, 4], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([4, 4, 4, 4, 5, 1, 3], device='cuda:0')\n",
      "negative indices: tensor([4, 4, 4, 4, 3, 4, 3], device='cuda:0')\n",
      "negative indices: tensor([2, 4, 6, 6, 5, 0, 3], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([3, 4, 4, 2, 5, 0, 2], device='cuda:0')\n",
      "negative indices: tensor([3, 2, 6, 6, 2, 4, 2], device='cuda:0')\n",
      "negative indices: tensor([5, 5, 4, 0, 2, 2, 2], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([2, 3, 0, 4, 5, 2, 5], device='cuda:0')\n",
      "negative indices: tensor([2, 5, 0, 5, 3, 3, 5], device='cuda:0')\n",
      "negative indices: tensor([2, 0, 5, 5, 3, 3, 4], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([4, 3, 6, 4, 0, 2, 1], device='cuda:0')\n",
      "negative indices: tensor([6, 3, 6, 4, 0, 1, 4], device='cuda:0')\n",
      "negative indices: tensor([6, 4, 4, 1, 6, 1, 4], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([4, 3, 0, 5, 0, 0, 0], device='cuda:0')\n",
      "negative indices: tensor([5, 3, 5, 1, 5, 6, 5], device='cuda:0')\n",
      "negative indices: tensor([5, 4, 5, 1, 5, 3, 0], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([2, 4, 4, 5, 1, 6, 5], device='cuda:0')\n",
      "negative indices: tensor([5, 4, 6, 5, 2, 3, 5], device='cuda:0')\n",
      "negative indices: tensor([5, 4, 6, 5, 1, 3, 3], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([5, 4, 5, 6, 6, 6, 4], device='cuda:0')\n",
      "negative indices: tensor([5, 0, 3, 1, 3, 1, 4], device='cuda:0')\n",
      "negative indices: tensor([5, 4, 4, 1, 2, 4, 1], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([3, 0, 3, 4, 3, 2, 4], device='cuda:0')\n",
      "negative indices: tensor([3, 5, 3, 0, 2, 3, 4], device='cuda:0')\n",
      "negative indices: tensor([6, 6, 6, 0, 0, 3, 1], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([4, 0, 3, 6, 2, 2, 3], device='cuda:0')\n",
      "negative indices: tensor([4, 0, 0, 4, 1, 3, 3], device='cuda:0')\n",
      "negative indices: tensor([5, 4, 4, 6, 1, 3, 3], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([1, 6, 0, 6, 1, 0, 0], device='cuda:0')\n",
      "negative indices: tensor([5, 6, 0, 5, 3, 0, 5], device='cuda:0')\n",
      "negative indices: tensor([2, 6, 6, 6, 6, 6, 1], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([4, 2, 5, 1, 3, 4, 1], device='cuda:0')\n",
      "negative indices: tensor([5, 6, 6, 4, 3, 0, 1], device='cuda:0')\n",
      "negative indices: tensor([3, 6, 6, 4, 5, 2, 5], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([1, 4, 5, 2, 6, 3, 3], device='cuda:0')\n",
      "negative indices: tensor([2, 4, 0, 1, 6, 1, 4], device='cuda:0')\n",
      "negative indices: tensor([2, 2, 5, 1, 5, 2, 4], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([5, 5, 3, 4, 5, 2, 3], device='cuda:0')\n",
      "negative indices: tensor([5, 5, 3, 2, 0, 6, 4], device='cuda:0')\n",
      "negative indices: tensor([3, 2, 6, 2, 3, 0, 2], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([1, 3, 4, 4, 5, 1, 5], device='cuda:0')\n",
      "negative indices: tensor([4, 5, 6, 4, 6, 6, 5], device='cuda:0')\n",
      "negative indices: tensor([1, 5, 5, 4, 6, 2, 1], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([6, 2, 3, 2, 1, 4, 0], device='cuda:0')\n",
      "negative indices: tensor([4, 6, 5, 4, 6, 4, 4], device='cuda:0')\n",
      "negative indices: tensor([4, 6, 1, 4, 1, 4, 0], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([3, 4, 3, 2, 5, 4, 3], device='cuda:0')\n",
      "negative indices: tensor([6, 5, 0, 2, 5, 4, 0], device='cuda:0')\n",
      "negative indices: tensor([6, 6, 3, 0, 5, 3, 2], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([4, 4, 4, 2, 1, 3, 1], device='cuda:0')\n",
      "negative indices: tensor([2, 4, 6, 5, 1, 4, 4], device='cuda:0')\n",
      "negative indices: tensor([2, 4, 4, 4, 0, 6, 4], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([1, 3, 5, 5, 3, 2, 1], device='cuda:0')\n",
      "negative indices: tensor([6, 3, 5, 0, 1, 0, 4], device='cuda:0')\n",
      "negative indices: tensor([5, 0, 0, 0, 5, 0, 4], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([1, 4, 0, 6, 1, 4, 4], device='cuda:0')\n",
      "negative indices: tensor([1, 4, 1, 0, 1, 4, 4], device='cuda:0')\n",
      "negative indices: tensor([2, 6, 5, 0, 5, 4, 3], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([6, 2, 3, 2, 0, 1, 0], device='cuda:0')\n",
      "negative indices: tensor([5, 2, 4, 2, 1, 6, 5], device='cuda:0')\n",
      "negative indices: tensor([5, 0, 4, 2, 1, 1, 5], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([3, 4, 5, 5, 1, 6, 5], device='cuda:0')\n",
      "negative indices: tensor([2, 4, 6, 4, 3, 0, 4], device='cuda:0')\n",
      "negative indices: tensor([6, 4, 6, 5, 3, 3, 5], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([5, 4, 3, 2, 5, 2, 3], device='cuda:0')\n",
      "negative indices: tensor([4, 4, 5, 6, 5, 2, 3], device='cuda:0')\n",
      "negative indices: tensor([4, 6, 3, 5, 6, 3, 0], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([2, 2, 5, 5, 0, 4, 2], device='cuda:0')\n",
      "negative indices: tensor([2, 2, 1, 4, 2, 0, 2], device='cuda:0')\n",
      "negative indices: tensor([2, 3, 0, 4, 1, 4, 1], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([1, 0, 6, 1, 5, 1, 1], device='cuda:0')\n",
      "negative indices: tensor([3, 5, 6, 4, 5, 0, 2], device='cuda:0')\n",
      "negative indices: tensor([1, 4, 6, 4, 0, 2, 2], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([1, 4, 5, 1, 2, 4, 5], device='cuda:0')\n",
      "negative indices: tensor([1, 2, 1, 4, 2, 2, 1], device='cuda:0')\n",
      "negative indices: tensor([1, 4, 5, 4, 2, 2, 1], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([5, 5, 3, 2, 6, 0, 4], device='cuda:0')\n",
      "negative indices: tensor([5, 2, 0, 4, 6, 0, 4], device='cuda:0')\n",
      "negative indices: tensor([4, 2, 0, 4, 1, 1, 4], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([2, 5, 0, 4, 3, 1, 4], device='cuda:0')\n",
      "negative indices: tensor([4, 6, 0, 5, 3, 3, 1], device='cuda:0')\n",
      "negative indices: tensor([2, 6, 4, 5, 3, 1, 1], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([1, 3, 0, 1, 1, 4, 2], device='cuda:0')\n",
      "negative indices: tensor([5, 3, 5, 0, 5, 1, 2], device='cuda:0')\n",
      "negative indices: tensor([5, 3, 5, 5, 2, 0, 1], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([6, 4, 5, 1, 1, 3, 3], device='cuda:0')\n",
      "negative indices: tensor([6, 4, 3, 6, 1, 2, 5], device='cuda:0')\n",
      "negative indices: tensor([6, 4, 5, 6, 1, 2, 5], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([5, 2, 6, 4, 3, 1, 5], device='cuda:0')\n",
      "negative indices: tensor([6, 6, 4, 5, 1, 6, 3], device='cuda:0')\n",
      "negative indices: tensor([4, 2, 4, 5, 5, 3, 0], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([6, 5, 5, 6, 5, 2, 3], device='cuda:0')\n",
      "negative indices: tensor([1, 4, 1, 0, 1, 0, 1], device='cuda:0')\n",
      "negative indices: tensor([1, 3, 1, 1, 0, 2, 1], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([6, 2, 1, 6, 5, 1, 5], device='cuda:0')\n",
      "negative indices: tensor([4, 5, 1, 1, 2, 4, 5], device='cuda:0')\n",
      "negative indices: tensor([4, 5, 5, 1, 3, 4, 5], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([2, 2, 1, 4, 3, 6, 1], device='cuda:0')\n",
      "negative indices: tensor([2, 2, 1, 4, 3, 4, 5], device='cuda:0')\n",
      "negative indices: tensor([2, 2, 4, 4, 3, 2, 4], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([1, 0, 6, 5, 1, 4, 1], device='cuda:0')\n",
      "negative indices: tensor([6, 6, 1, 5, 1, 4, 1], device='cuda:0')\n",
      "negative indices: tensor([1, 4, 1, 5, 5, 4, 1], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([1, 2, 1, 4, 6, 1, 4], device='cuda:0')\n",
      "negative indices: tensor([1, 5, 1, 4, 3, 2, 1], device='cuda:0')\n",
      "negative indices: tensor([1, 2, 1, 4, 3, 1, 3], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([2, 4, 6, 4, 5, 0, 5], device='cuda:0')\n",
      "negative indices: tensor([5, 3, 0, 1, 5, 1, 0], device='cuda:0')\n",
      "negative indices: tensor([5, 3, 1, 1, 1, 4, 5], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([6, 5, 0, 5, 3, 1, 4], device='cuda:0')\n",
      "negative indices: tensor([2, 2, 0, 4, 6, 1, 5], device='cuda:0')\n",
      "negative indices: tensor([2, 2, 1, 4, 6, 1, 0], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([6, 4, 5, 4, 1, 0, 0], device='cuda:0')\n",
      "negative indices: tensor([5, 0, 5, 2, 1, 0, 0], device='cuda:0')\n",
      "negative indices: tensor([1, 0, 4, 2, 1, 6, 4], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([3, 4, 3, 2, 6, 6, 4], device='cuda:0')\n",
      "negative indices: tensor([3, 4, 3, 2, 6, 6, 4], device='cuda:0')\n",
      "negative indices: tensor([6, 4, 3, 2, 6, 6, 4], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([2, 4, 0, 1, 2, 2, 2], device='cuda:0')\n",
      "negative indices: tensor([2, 3, 1, 1, 6, 2, 2], device='cuda:0')\n",
      "negative indices: tensor([2, 3, 1, 1, 1, 1, 3], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([4, 6, 1, 5, 1, 3, 1], device='cuda:0')\n",
      "negative indices: tensor([4, 2, 6, 5, 6, 3, 0], device='cuda:0')\n",
      "negative indices: tensor([5, 6, 3, 5, 5, 3, 0], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([1, 6, 5, 2, 6, 6, 2], device='cuda:0')\n",
      "negative indices: tensor([6, 6, 5, 2, 6, 3, 4], device='cuda:0')\n",
      "negative indices: tensor([6, 6, 3, 2, 5, 4, 4], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([1, 6, 4, 2, 5, 1, 5], device='cuda:0')\n",
      "negative indices: tensor([1, 0, 4, 6, 2, 1, 3], device='cuda:0')\n",
      "negative indices: tensor([1, 5, 5, 5, 3, 3, 5], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([4, 5, 4, 6, 5, 2, 3], device='cuda:0')\n",
      "negative indices: tensor([2, 5, 4, 6, 5, 4, 3], device='cuda:0')\n",
      "negative indices: tensor([6, 3, 4, 5, 5, 3, 3], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([3, 4, 4, 5, 3, 3, 5], device='cuda:0')\n",
      "negative indices: tensor([6, 2, 4, 5, 6, 6, 5], device='cuda:0')\n",
      "negative indices: tensor([6, 4, 4, 5, 0, 3, 4], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([2, 4, 1, 1, 1, 4, 5], device='cuda:0')\n",
      "negative indices: tensor([4, 2, 1, 6, 1, 1, 4], device='cuda:0')\n",
      "negative indices: tensor([4, 3, 6, 1, 1, 0, 1], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([1, 2, 5, 6, 1, 1, 2], device='cuda:0')\n",
      "negative indices: tensor([4, 0, 5, 6, 0, 4, 3], device='cuda:0')\n",
      "negative indices: tensor([5, 0, 0, 6, 0, 0, 3], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([6, 6, 4, 1, 2, 2, 2], device='cuda:0')\n",
      "negative indices: tensor([6, 2, 0, 6, 0, 6, 0], device='cuda:0')\n",
      "negative indices: tensor([6, 4, 0, 6, 0, 2, 5], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([6, 3, 4, 1, 2, 1, 0], device='cuda:0')\n",
      "negative indices: tensor([2, 0, 6, 1, 6, 0, 0], device='cuda:0')\n",
      "negative indices: tensor([1, 0, 1, 5, 6, 4, 2], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([3, 4, 0, 4, 5, 3, 3], device='cuda:0')\n",
      "negative indices: tensor([3, 4, 0, 5, 5, 0, 3], device='cuda:0')\n",
      "negative indices: tensor([3, 4, 1, 5, 2, 0, 3], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([2, 6, 0, 0, 3, 4, 1], device='cuda:0')\n",
      "negative indices: tensor([3, 4, 0, 2, 6, 2, 0], device='cuda:0')\n",
      "negative indices: tensor([3, 5, 0, 6, 5, 4, 0], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([1, 2, 1, 1, 1, 3, 3], device='cuda:0')\n",
      "negative indices: tensor([4, 3, 3, 1, 3, 1, 1], device='cuda:0')\n",
      "negative indices: tensor([4, 3, 3, 1, 6, 2, 5], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([5, 6, 0, 6, 1, 6, 1], device='cuda:0')\n",
      "negative indices: tensor([5, 6, 0, 4, 1, 3, 4], device='cuda:0')\n",
      "negative indices: tensor([5, 4, 0, 4, 1, 6, 4], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([5, 4, 6, 0, 0, 2, 5], device='cuda:0')\n",
      "negative indices: tensor([4, 5, 5, 4, 1, 2, 5], device='cuda:0')\n",
      "negative indices: tensor([4, 4, 4, 6, 1, 6, 2], device='cuda:0')\n",
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([4, 6, 3, 5, 1, 2, 1], device='cuda:0')\n",
      "negative indices: tensor([6, 6, 3, 5, 3, 6, 4], device='cuda:0')\n",
      "negative indices: tensor([6, 4, 5, 6, 6, 3, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | 0/150 [01:23<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voxel_list[0].shape torch.Size([1, 1, 14278])\n",
      "matched_voxel_iter shape:  torch.Size([1, 2, 17907])\n",
      "matched_voxel_list1.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "matched_voxel_list2.shape [torch.Size([1, 1, 14278]), torch.Size([1, 1, 15226]), torch.Size([1, 1, 13153]), torch.Size([1, 1, 13039]), torch.Size([1, 1, 17907]), torch.Size([1, 1, 12682]), torch.Size([1, 1, 14386])]\n",
      "negative indices: tensor([5, 5, 6, 2, 0, 6, 5], device='cuda:0')\n",
      "negative indices: tensor([1, 0, 5, 1, 0, 6, 2], device='cuda:0')\n",
      "negative indices: tensor([5, 5, 3, 1, 0, 1, 2], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 243\u001b[0m\n\u001b[1;32m    240\u001b[0m         blurry_pixcorr \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pixcorr\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    242\u001b[0m utils\u001b[38;5;241m.\u001b[39mcheck_loss(loss)\n\u001b[0;32m--> 243\u001b[0m \u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    246\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/export/raid1/home/kneel027/miniconda3/envs/mindeye2-rel/lib/python3.11/site-packages/accelerate/accelerator.py:1987\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1986\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1987\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1988\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1989\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/export/raid1/home/kneel027/miniconda3/envs/mindeye2-rel/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/export/raid1/home/kneel027/miniconda3/envs/mindeye2-rel/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "progress_bar = tqdm(range(epoch,num_epochs), ncols=1200, disable=(local_rank!=0))\n",
    "test_image, test_voxel = None, None\n",
    "mse = nn.MSELoss()\n",
    "l1 = nn.L1Loss()\n",
    "tml = nn.TripletMarginLoss(margin=1.0).to(device)\n",
    "soft_loss_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    model.train()\n",
    "\n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    test_fwd_percent_correct = 0.\n",
    "    test_bwd_percent_correct = 0.\n",
    "    \n",
    "    recon_cossim = 0.\n",
    "    test_recon_cossim = 0.\n",
    "    recon_mse = 0.\n",
    "    test_recon_mse = 0.\n",
    "\n",
    "    loss_clip_total = 0.\n",
    "    loss_contrastive_total = 0.\n",
    "    loss_blurry_total = 0.\n",
    "    loss_blurry_cont_total = 0.\n",
    "    test_loss_clip_total = 0.\n",
    "    test_loss_contrastive_total = 0.\n",
    "    \n",
    "    loss_prior_total = 0.\n",
    "    test_loss_prior_total = 0.\n",
    "\n",
    "    blurry_pixcorr = 0.\n",
    "    test_blurry_pixcorr = 0. # needs >.456 to beat low-level subj01 results in mindeye v1\n",
    "\n",
    "    # pre-load all batches for this epoch (it's MUCH faster to pre-load in bulk than to separate loading per batch)\n",
    "    voxel_iters = {} # empty dict because diff subjects have differing # of voxels\n",
    "    image_iters = torch.zeros(num_iterations_per_epoch, batch_size*len(subj_list), 3, 224, 224).float()\n",
    "    annot_iters = {}\n",
    "    perm_iters, betas_iters, select_iters = {}, {}, {}\n",
    "    for s, train_dl in enumerate(train_dls):\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            for iter, (behav0, past_behav0, future_behav0, old_behav0) in enumerate(train_dl):    \n",
    "                image0 = images[behav0[:,0,0].cpu().long()].float()\n",
    "                image_iters[iter,s*batch_size:s*batch_size+batch_size] = image0\n",
    "                \n",
    "                voxel0 = voxels[f'subj0{subj_list[s]}'][behav0[:,0,5].cpu().long()]\n",
    "                voxel0 = torch.Tensor(voxel0)\n",
    "                \n",
    "                old_behavior = old_behav0[:,:2,5].cpu().long()\n",
    "                old_voxel0 = voxels[f'subj0{subj_list[s]}'][old_behavior]\n",
    "                \n",
    "                voxel_iters[f\"subj0{subj_list[s]}_iter{iter}_trial_reps\"] = old_voxel0\n",
    "                \n",
    "                if seq_len==1:\n",
    "                    voxel0 = voxel0.unsqueeze(1)\n",
    "                else:\n",
    "                    if seq_past>0:\n",
    "                        past_behavior = past_behav0[:,:(seq_past),5].cpu().long()\n",
    "                        past_voxel0 = voxels[f'subj0{subj_list[s]}'][past_behavior]\n",
    "                        past_voxel0[past_behavior==-1] = voxel0[torch.where(past_behavior==-1)[0]] # replace invalid past voxels \n",
    "                        past_voxel0 = torch.Tensor(past_voxel0)\n",
    "\n",
    "                        # if shared1000, then you need to mask it out \n",
    "                        for p in range(seq_past):\n",
    "                            mask = (past_behav0[:,p,-1] == 1) # [16,] bool\n",
    "                            index = torch.nonzero(mask.cpu()).squeeze()\n",
    "                            past_voxel0[index,p,:] = torch.zeros_like(past_voxel0[index,p,:])\n",
    "\n",
    "                    if seq_future>0:\n",
    "                        future_behavior = future_behav0[:,:(seq_future),5].cpu().long()\n",
    "                        future_voxel0 = voxels[f'subj0{subj_list[s]}'][future_behavior]\n",
    "                        future_voxel0[future_behavior==-1] = voxel0[torch.where(future_behavior==-1)[0]] # replace invalid past voxels \n",
    "                        future_voxel0 = torch.Tensor(future_voxel0)\n",
    "\n",
    "                        # if shared1000, then you need to mask it out \n",
    "                        for p in range(seq_future):\n",
    "                            mask = (future_behav0[:,p,-1] == 1) # [16,] bool\n",
    "                            index = torch.nonzero(mask.cpu()).squeeze()\n",
    "                            future_voxel0[index,p,:] = torch.zeros_like(future_voxel0[index,p,:])\n",
    "\n",
    "                    # concatenate current timepoint with past/future\n",
    "                    if seq_past > 0 and seq_future > 0:\n",
    "                        voxel0 = torch.cat((voxel0.unsqueeze(1), past_voxel0), axis=1)\n",
    "                        voxel0 = torch.cat((voxel0, future_voxel0), axis=1)\n",
    "                    elif seq_past > 0:\n",
    "                        voxel0 = torch.cat((voxel0.unsqueeze(1), past_voxel0), axis=1)\n",
    "                    else:\n",
    "                        voxel0 = torch.cat((voxel0.unsqueeze(1), future_voxel0), axis=1)\n",
    "\n",
    "                if epoch < int(mixup_pct * num_epochs):\n",
    "                    voxel0, perm, betas, select = utils.mixco(voxel0)\n",
    "                    perm_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = perm\n",
    "                    betas_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = betas\n",
    "                    select_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = select\n",
    "\n",
    "                voxel_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = voxel0\n",
    "                \n",
    "                if iter >= num_iterations_per_epoch-1:\n",
    "                    break\n",
    "\n",
    "    # you now have voxel_iters and image_iters with num_iterations_per_epoch batches each\n",
    "    for train_i in range(num_iterations_per_epoch):\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            optimizer.zero_grad()\n",
    "            loss=0.\n",
    "\n",
    "            voxel_list = [voxel_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "            matched_voxel_list_1 = [voxel_iters[f\"subj0{s}_iter{train_i}_trial_reps\"][:,0].unsqueeze(1).detach().to(device) for s in subj_list]\n",
    "            matched_voxel_list_2 = [voxel_iters[f\"subj0{s}_iter{train_i}_trial_reps\"][:,1].unsqueeze(1).detach().to(device) for s in subj_list]\n",
    "            image = image_iters[train_i].detach()\n",
    "            image = image.to(device)\n",
    "            if use_image_aug: \n",
    "                image = img_augment(image)\n",
    "\n",
    "            clip_target = clip_img_embedder(image)\n",
    "            assert not torch.any(torch.isnan(clip_target))\n",
    "\n",
    "            if epoch < int(mixup_pct * num_epochs):\n",
    "                perm_list = [perm_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                perm = torch.cat(perm_list, dim=0)\n",
    "                betas_list = [betas_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                betas = torch.cat(betas_list, dim=0)\n",
    "                select_list = [select_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                select = torch.cat(select_list, dim=0)\n",
    "\n",
    "            voxel_ridge_list = [model.ridge(voxel_list[si],si) for si,s in enumerate(subj_list)]\n",
    "            voxel_ridge = torch.cat(voxel_ridge_list, dim=0)\n",
    "\n",
    "            backbone, clip_voxels, blurry_image_enc_ = model.backbone(voxel_ridge)\n",
    "            if contrastive_scale>0:\n",
    "                #Do it 2X again for the matched trial voxels\n",
    "                matched_voxel_ridge_list_1 = [model.ridge(matched_voxel_list_1[si],si) for si,s in enumerate(subj_list)]\n",
    "                matched_voxel_ridge_1 = torch.cat(matched_voxel_ridge_list_1, dim=0)\n",
    "\n",
    "                matched_backbone_1, matched_clip_voxels_1, matched_blurry_image_enc__1 = model.backbone(matched_voxel_ridge_1)\n",
    "                \n",
    "                matched_voxel_ridge_list_2 = [model.ridge(matched_voxel_list_2[si],si) for si,s in enumerate(subj_list)]\n",
    "                matched_voxel_ridge_2 = torch.cat(matched_voxel_ridge_list_2, dim=0)\n",
    "\n",
    "                matched_backbone_2, matched_clip_voxels_2, matched_blurry_image_enc__2 = model.backbone(matched_voxel_ridge_2)\n",
    "\n",
    "            if clip_scale>0:\n",
    "                clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "                \n",
    "            if contrastive_scale>0:\n",
    "                matched_clip_voxels_1_norm = nn.functional.normalize(matched_clip_voxels_1.flatten(1), dim=-1)\n",
    "                matched_clip_voxels_2_norm = nn.functional.normalize(matched_clip_voxels_2.flatten(1), dim=-1)\n",
    "                embeddings = [clip_voxels_norm, matched_clip_voxels_1_norm, matched_clip_voxels_2_norm]\n",
    "                loss_contrastive = 0\n",
    "                # For each pair of trials, use one as anchor and the other as positive\n",
    "                trial_pairs = list(combinations(range(3), 2))\n",
    "                for anchor_trial, positive_trial in trial_pairs:\n",
    "                    anchor = embeddings[anchor_trial]\n",
    "                    positive = embeddings[positive_trial]\n",
    "                    \n",
    "                    # Compute distances on batch samples to find \"hard\" negatives\n",
    "                    distances = torch.cdist(embeddings[anchor_trial], embeddings[positive_trial]).squeeze(0)\n",
    "                    distances.fill_diagonal_(float('inf'))\n",
    "                    _, negative_indices = distances.min(dim=1)\n",
    "                    negative = embeddings[positive_trial][negative_indices]\n",
    "\n",
    "                    # Compute loss\n",
    "                    # anchor is batch_size * embedding_dim corresponding to the embedded vectors for an set of anchor patterns of brain activity\n",
    "                    # positive is batch_size * embedding_dim corresponding to the embedding vectors for a set of brain activity patterns that are for the same stimuli as the anchor, but a different repetition\n",
    "                    # negative is batch_size * embedding_dim corresponding to the embedding vectors for a set of brain activity patterns that are the closest in the batch to the anchor patterns, without being positive samples\n",
    "                    loss_contrastive += tml(anchor, positive, negative)\n",
    "                loss_contrastive *= contrastive_scale\n",
    "                loss_contrastive_total += loss_contrastive.item()\n",
    "                loss += loss_contrastive\n",
    "                \n",
    "            if use_prior:\n",
    "                loss_prior, prior_out = model.diffusion_prior(text_embed=backbone, image_embed=clip_target)\n",
    "                loss_prior_total += loss_prior.item()\n",
    "                loss_prior *= prior_scale\n",
    "                loss += loss_prior\n",
    "\n",
    "                recon_cossim += nn.functional.cosine_similarity(prior_out, clip_target).mean().item()\n",
    "                recon_mse += mse(prior_out, clip_target).item()\n",
    "\n",
    "            if clip_scale>0:\n",
    "                if epoch < int(mixup_pct * num_epochs):                \n",
    "                    loss_clip = utils.mixco_nce(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=.006,\n",
    "                        perm=perm, betas=betas, select=select)\n",
    "                else:\n",
    "                    epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]\n",
    "                    loss_clip = utils.soft_clip_loss(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=epoch_temp)\n",
    "\n",
    "                loss_clip_total += loss_clip.item()\n",
    "                loss_clip *= clip_scale\n",
    "                loss += loss_clip\n",
    "\n",
    "            if blurry_recon:     \n",
    "                image_enc_pred, transformer_feats = blurry_image_enc_\n",
    "\n",
    "                image_enc = autoenc.encode(2*image-1).latent_dist.mode() * 0.18215\n",
    "                loss_blurry = l1(image_enc_pred, image_enc)\n",
    "                loss_blurry_total += loss_blurry.item()\n",
    "\n",
    "                if epoch < int(mixup_pct * num_epochs):\n",
    "                    image_enc_shuf = image_enc[perm]\n",
    "                    betas_shape = [-1] + [1]*(len(image_enc.shape)-1)\n",
    "                    image_enc[select] = image_enc[select] * betas[select].reshape(*betas_shape) + \\\n",
    "                        image_enc_shuf[select] * (1 - betas[select]).reshape(*betas_shape)\n",
    "\n",
    "                image_norm = (image - mean)/std\n",
    "                image_aug = (blur_augs(image) - mean)/std\n",
    "                _, cnx_embeds = cnx(image_norm)\n",
    "                _, cnx_aug_embeds = cnx(image_aug)\n",
    "\n",
    "                cont_loss = utils.soft_cont_loss(\n",
    "                    nn.functional.normalize(transformer_feats.reshape(-1, transformer_feats.shape[-1]), dim=-1),\n",
    "                    nn.functional.normalize(cnx_embeds.reshape(-1, cnx_embeds.shape[-1]), dim=-1),\n",
    "                    nn.functional.normalize(cnx_aug_embeds.reshape(-1, cnx_embeds.shape[-1]), dim=-1),\n",
    "                    temp=0.2)\n",
    "                loss_blurry_cont_total += cont_loss.item()\n",
    "\n",
    "                loss += (loss_blurry + 0.1*cont_loss) * blur_scale #/.18215\n",
    "\n",
    "            if clip_scale>0:\n",
    "                # forward and backward top 1 accuracy        \n",
    "                labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "                fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "\n",
    "            if blurry_recon:\n",
    "                with torch.no_grad():\n",
    "                    # only doing pixcorr eval on a subset of the samples per batch because its costly & slow to compute autoenc.decode()\n",
    "                    random_samps = np.random.choice(np.arange(len(image)), size=len(image)//5, replace=False)\n",
    "                    blurry_recon_images = (autoenc.decode(image_enc_pred[random_samps]/0.18215).sample/ 2 + 0.5).clamp(0,1)\n",
    "                    pixcorr = utils.pixcorr(image[random_samps], blurry_recon_images)\n",
    "                    blurry_pixcorr += pixcorr.item()\n",
    "\n",
    "            utils.check_loss(loss)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            if lr_scheduler_type is not None:\n",
    "                lr_scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    if local_rank==0:\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=data_type): \n",
    "            for test_i, (behav, past_behav, future_behav, old_behav) in enumerate(test_dl):  \n",
    "                # all test samples should be loaded per batch such that test_i should never exceed 0\n",
    "                assert len(behav) == num_test\n",
    "\n",
    "                ## Average same-image repeats ##\n",
    "                if test_image is None:\n",
    "                    voxel = voxels[f'subj0{subj}'][behav[:,0,5].cpu().long()]\n",
    "                    \n",
    "                    if seq_len==1:\n",
    "                        voxel = voxel.unsqueeze(1)\n",
    "                    else:\n",
    "                        if seq_past>0:\n",
    "                            past_behavior = past_behav[:,:(seq_past),5].cpu().long()\n",
    "                            past_voxels = voxels[f'subj0{subj}'][past_behavior]\n",
    "                            if torch.any(past_behavior==-1).item(): # remove invalid voxels (-1 if there is no timepoint available)\n",
    "                                past_voxels[torch.where(past_behavior==-1)[0]] = 0\n",
    "\n",
    "                        if seq_future>0:\n",
    "                            future_behavior = future_behav[:,:(seq_future),5].cpu().long()\n",
    "                            future_voxels = voxels[f'subj0{subj}'][future_behavior]                    \n",
    "                            if torch.any(future_behavior==-1).item(): # remove invalid voxels (-1 if there is no timepoint available)\n",
    "                                future_voxels[torch.where(future_behavior==-1)[0]] = 0\n",
    "                            \n",
    "                        if seq_past > 0 and seq_future > 0:\n",
    "                            voxel = torch.cat((voxel.unsqueeze(1), past_voxels), axis=1)\n",
    "                            voxel = torch.cat((voxel, future_voxels), axis=1)\n",
    "                        elif seq_past > 0:\n",
    "                            voxel = torch.cat((voxel.unsqueeze(1), past_voxels), axis=1)\n",
    "                        else:\n",
    "                            voxel = torch.cat((voxel.unsqueeze(1), future_voxels), axis=1)\n",
    "\n",
    "                    image = behav[:,0,0].cpu().long()\n",
    "\n",
    "                    unique_image, sort_indices = torch.unique(image, return_inverse=True)\n",
    "                    for im in unique_image:\n",
    "                        locs = torch.where(im == image)[0]\n",
    "                        if len(locs)==1:\n",
    "                            locs = locs.repeat(3)\n",
    "                        elif len(locs)==2:\n",
    "                            locs = locs.repeat(2)[:3]\n",
    "                        assert len(locs)==3\n",
    "                        if test_image is None:\n",
    "                            test_image = images[im][None]\n",
    "                            test_voxel = voxel[locs][None]\n",
    "                        else:\n",
    "                            test_image = torch.vstack((test_image, images[im][None]))\n",
    "                            test_voxel = torch.vstack((test_voxel, voxel[locs][None]))\n",
    "\n",
    "                loss=0.\n",
    "                            \n",
    "                test_indices = torch.arange(len(test_voxel))[:300]\n",
    "                voxel = test_voxel[test_indices].to(device)\n",
    "                image = test_image[test_indices].to(device)\n",
    "                assert len(image) == 300\n",
    "\n",
    "                clip_target = clip_img_embedder(image.float())\n",
    "                embeddings = []\n",
    "                for rep in range(3):\n",
    "                    voxel_ridge = model.ridge(voxel[:,rep],0) # 0th index of subj_list\n",
    "                    backbone0, clip_voxels0, blurry_image_enc_ = model.backbone(voxel_ridge)\n",
    "                    embeddings.append(clip_voxels0)\n",
    "                    if rep==0:\n",
    "                        clip_voxels = clip_voxels0\n",
    "                        backbone = backbone0\n",
    "                    else:\n",
    "                        clip_voxels += clip_voxels0\n",
    "                        backbone += backbone0\n",
    "                clip_voxels /= 3\n",
    "                backbone /= 3\n",
    "                \n",
    "\n",
    "            if clip_scale>0:\n",
    "                clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "                \n",
    "            if contrastive_scale>0:\n",
    "                for rep in range(len(embeddings)):\n",
    "                    embeddings[rep] = nn.functional.normalize(embeddings[rep].flatten(1), dim=-1)\n",
    "                    \n",
    "                loss_contrastive = 0\n",
    "                # For each pair of trials, use one as anchor and the other as positive\n",
    "                trial_pairs = list(combinations(range(3), 2))\n",
    "                for anchor_trial, positive_trial in trial_pairs:\n",
    "                    anchor = embeddings[anchor_trial]\n",
    "                    positive = embeddings[positive_trial]\n",
    "                    \n",
    "                    # Compute distances on batch samples to find \"hard\" negatives\n",
    "                    distances = torch.cdist(embeddings[anchor_trial], embeddings[positive_trial]).squeeze(0)\n",
    "                    distances.fill_diagonal_(float('inf'))\n",
    "                    _, negative_indices = distances.min(dim=1)\n",
    "                    print(negative_indices)\n",
    "                    negative = embeddings[positive_trial][negative_indices]\n",
    "\n",
    "                    # Compute loss\n",
    "                    # anchor is batch_size * embedding_dim corresponding to the embedded vectors for an set of anchor patterns of brain activity\n",
    "                    # positive is batch_size * embedding_dim corresponding to the embedding vectors for a set of brain activity patterns that are for the same stimuli as the anchor, but a different repetition\n",
    "                    # negative is batch_size * embedding_dim corresponding to the embedding vectors for a set of brain activity patterns that are the closest in the batch to the anchor patterns, without being positive samples\n",
    "                    loss_contrastive += tml(anchor, positive, negative)\n",
    "                loss_contrastive *= contrastive_scale\n",
    "                test_loss_contrastive_total += loss_contrastive.item()\n",
    "                loss += loss_contrastive\n",
    "\n",
    "                if clip_scale>0:\n",
    "                    clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                    clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "                \n",
    "                # for some evals, only doing a subset of the samples per batch because of computational cost\n",
    "                random_samps = np.random.choice(np.arange(len(image)), size=len(image)//5, replace=False)\n",
    "                \n",
    "                if use_prior:\n",
    "                    loss_prior, contaminated_prior_out = model.diffusion_prior(text_embed=backbone[random_samps], image_embed=clip_target[random_samps])\n",
    "                    test_loss_prior_total += loss_prior.item()\n",
    "                    loss_prior *= prior_scale\n",
    "                    loss += loss_prior\n",
    "                    \n",
    "                    if visualize_prior:\n",
    "                        # now get unCLIP prediction without feeding it the image embed to get uncontaminated reconstruction\n",
    "                        prior_out = model.diffusion_prior.p_sample_loop(backbone[random_samps].shape, \n",
    "                                        text_cond = dict(text_embed = backbone[random_samps]), \n",
    "                                        cond_scale = 1., timesteps = 20)\n",
    "\n",
    "                        test_recon_cossim += nn.functional.cosine_similarity(prior_out, clip_target[random_samps]).mean().item()\n",
    "                        test_recon_mse += mse(prior_out, clip_target[random_samps]).item()\n",
    "                        \n",
    "                if clip_scale>0:\n",
    "                    loss_clip = utils.soft_clip_loss(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=.006)\n",
    "\n",
    "                    test_loss_clip_total += loss_clip.item()\n",
    "                    loss_clip = loss_clip * clip_scale\n",
    "                    loss += loss_clip\n",
    "\n",
    "                if blurry_recon:\n",
    "                    image_enc_pred, _ = blurry_image_enc_\n",
    "                    blurry_recon_images = (autoenc.decode(image_enc_pred[random_samps]/0.18215).sample / 2 + 0.5).clamp(0,1)\n",
    "                    pixcorr = utils.pixcorr(image[random_samps], blurry_recon_images)\n",
    "                    test_blurry_pixcorr += pixcorr.item()\n",
    "\n",
    "                if clip_scale>0:\n",
    "                    # forward and backward top 1 accuracy        \n",
    "                    labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "                    test_fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                    test_bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "                \n",
    "                utils.check_loss(loss)                \n",
    "                test_losses.append(loss.item())\n",
    "\n",
    "            # if utils.is_interactive(): clear_output(wait=True)\n",
    "            print(\"---\")\n",
    "\n",
    "            assert (test_i+1) == 1\n",
    "            logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "                \"test/loss\": np.mean(test_losses[-(test_i+1):]),\n",
    "                \"train/lr\": lrs[-1],\n",
    "                \"train/num_steps\": len(losses),\n",
    "                \"test/num_steps\": len(test_losses),\n",
    "                \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "                \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "                \"test/test_fwd_pct_correct\": test_fwd_percent_correct / (test_i + 1),\n",
    "                \"test/test_bwd_pct_correct\": test_bwd_percent_correct / (test_i + 1),\n",
    "                \"train/loss_clip_total\": loss_clip_total / (train_i + 1),\n",
    "                \"train/loss_contrastive_total\": loss_contrastive_total / (train_i + 1),\n",
    "                \"train/loss_blurry_total\": loss_blurry_total / (train_i + 1),\n",
    "                \"train/loss_blurry_cont_total\": loss_blurry_cont_total / (train_i + 1),\n",
    "                \"test/loss_clip_total\": test_loss_clip_total / (test_i + 1),\n",
    "                \"test/loss_contrastive_total\": test_loss_contrastive_total / (train_i + 1),\n",
    "                \"train/blurry_pixcorr\": blurry_pixcorr / (train_i + 1),\n",
    "                \"test/blurry_pixcorr\": test_blurry_pixcorr / (test_i + 1),\n",
    "                \"train/recon_cossim\": recon_cossim / (train_i + 1),\n",
    "                \"test/recon_cossim\": test_recon_cossim / (test_i + 1),\n",
    "                \"train/recon_mse\": recon_mse / (train_i + 1),\n",
    "                \"test/recon_mse\": test_recon_mse / (test_i + 1),\n",
    "                \"train/loss_prior\": loss_prior_total / (train_i + 1),\n",
    "                \"test/loss_prior\": test_loss_prior_total / (test_i + 1),\n",
    "                }\n",
    "\n",
    "            # if finished training, save jpg recons if they exist\n",
    "            if (epoch == num_epochs-1) or (epoch % ckpt_interval == 0):\n",
    "                if blurry_recon:    \n",
    "                    image_enc = autoenc.encode(2*image[:4]-1).latent_dist.mode() * 0.18215\n",
    "                    # transform blurry recon latents to images and plot it\n",
    "                    fig, axes = plt.subplots(1, 8, figsize=(10, 4))\n",
    "                    jj=-1\n",
    "                    for j in [0,1,2,3]:\n",
    "                        jj+=1\n",
    "                        axes[jj].imshow(utils.torch_to_Image((autoenc.decode(image_enc[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                        axes[jj].axis('off')\n",
    "                        jj+=1\n",
    "                        axes[jj].imshow(utils.torch_to_Image((autoenc.decode(image_enc_pred[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                        axes[jj].axis('off')\n",
    "\n",
    "                    if wandb_log:\n",
    "                        logs[f\"test/blur_recons\"] = wandb.Image(fig, caption=f\"epoch{epoch:03d}\")\n",
    "                        plt.close()\n",
    "                    else:\n",
    "                        plt.show()\n",
    "                        \n",
    "                if use_prior and visualize_prior: # output recons every ckpt\n",
    "                    idx = np.random.randint(0, 3)\n",
    "                    print(f\"reconstructing... idx={idx}\")\n",
    "                    samples = utils.unclip_recon(prior_out[[idx]],\n",
    "                             diffusion_engine,\n",
    "                             vector_suffix)\n",
    "                    if wandb_log:\n",
    "                        logs[f\"test/orig\"] = wandb.Image(transforms.ToPILImage()(image[idx]),\n",
    "                                                           caption=f\"epoch{epoch:03d}\")\n",
    "                        logs[f\"test/recons\"] = wandb.Image(transforms.ToPILImage()(samples[0]),\n",
    "                                                           caption=f\"epoch{epoch:03d}\")\n",
    "                    if utils.is_interactive():\n",
    "                        plt.figure(figsize=(2,2))\n",
    "                        plt.imshow(transforms.ToPILImage()(image[idx]))\n",
    "                        plt.axis('off')\n",
    "                        plt.show()\n",
    "                        \n",
    "                        plt.figure(figsize=(2,2))\n",
    "                        plt.imshow(transforms.ToPILImage()(samples[0]))\n",
    "                        plt.axis('off')\n",
    "                        plt.show()\n",
    "\n",
    "            progress_bar.set_postfix(**logs)\n",
    "\n",
    "            if wandb_log: wandb.log(logs)\n",
    "            \n",
    "    # Save model checkpoint and reconstruct\n",
    "    if (ckpt_saving) and (epoch % ckpt_interval == 0):\n",
    "        save_ckpt(f'last')\n",
    "\n",
    "    # wait for other GPUs to catch up if needed\n",
    "    accelerator.wait_for_everyone()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n===Finished!===\\n\")\n",
    "if ckpt_saving:\n",
    "    save_ckpt(f'last')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e81ae3-171f-40ad-a3e8-24bee4472325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()\n",
    "plt.plot(test_losses)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "62aae01ef0cf7b6af841ab1c8ce59175c4332e693ab3d00bc32ceffb78a35376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
